{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Spark SQL Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all scripts executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"chmod: changing permissions of './P1/spark_sql_p1.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P2/spark_sql_p2.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P3/spark_sql_p3.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P4/spark_sql_p4.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P5/spark_sql_p5.py': Operation not permitted\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!chmod a+x ./*/*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./*/*.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P1/spark_sql_p1.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( state_name = arr[24], site_num = arr[2], state_code = arr[0], county_code = arr[1]))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    query = \"SELECT state_name, \\\n",
    "        COUNT(state_name) \\\n",
    "        FROM (SELECT site_num, \\\n",
    "            CONCAT(state_code,county_code) as county, \\\n",
    "            state_name \\\n",
    "            FROM log GROUP BY county, site_num \\\n",
    "        ) \\\n",
    "        GROUP BY state_name \\\n",
    "        ORDER BY rank DESC\"\n",
    "\n",
    "    final = spark.sql(query)\n",
    "\n",
    "    final.show(30)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/22 17:43:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m14.592s',\n",
       " 'user\\t0m1.552s',\n",
       " 'sys\\t0m1.200s']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P1/spark_sql_p1.py > ./P1/p1.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"expression 'log.`state_name`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\\n'Sort ['rank DESC NULLS LAST], true\\n+- Aggregate [state_name#3], [state_name#3, count(state_name#3) AS count(state_name)#10L]\\n   +- SubqueryAlias `__auto_generated_subquery_name`\\n      +- Aggregate [concat(state_code#2, county_code#0), site_num#1], [site_num#1, concat(state_code#2, county_code#0) AS county#8, state_name#3]\\n         +- SubqueryAlias `log`\\n            +- LogicalRDD [county_code#0, site_num#1, state_code#2, state_name#3], false\\n\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P1/p1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P2/spark_sql_p2.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( State = arr[24], County = arr[25], countyCode = arr[1], Arithmetic_mean = float(arr[16])))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    stateRanksDF = spark.sql(\"SELECT State, \\\n",
    "        countyCode AS County_Code, \\\n",
    "        County, \\\n",
    "        AVG(Arithmetic_mean) AS Pollutant_levels \\\n",
    "        FROM log GROUP BY State, countyCode, County \\\n",
    "        ORDER BY Pollutant_levels DESC\")\n",
    "    stateRanksDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/22 17:40:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 2:============================================>              (3 + 1) / 4]',\n",
       " '[Stage 3:==========================>                             (93 + 9) / 200]',\n",
       " '[Stage 3:======================================>                (139 + 8) / 200]',\n",
       " '[Stage 3:==================================================>    (184 + 9) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m16.081s',\n",
       " 'user\\t0m1.157s',\n",
       " 'sys\\t0m0.934s']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P2/spark_sql_p2.py > ./P2/p2.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+--------------------+------------------+\r\n",
      "|            State|County_Code|              County|  Pollutant_levels|\r\n",
      "+-----------------+-----------+--------------------+------------------+\r\n",
      "|        Tennessee|        167|              Tipton|            2556.0|\r\n",
      "|         New York|        059|              Nassau|              19.0|\r\n",
      "|             Ohio|        029|          Columbiana| 7.385690735785953|\r\n",
      "|          Montana|        067|                Park| 5.611212121212121|\r\n",
      "|Country Of Mexico|        006|     CHIHUAHUA STATE|         4.5121875|\r\n",
      "|   North Carolina|        027|            Caldwell| 4.116666666666667|\r\n",
      "|       California|        031|               Kings|3.9843770491803276|\r\n",
      "|       California|        039|              Madera|            3.7393|\r\n",
      "|   North Carolina|        069|            Franklin|3.3499999999999996|\r\n",
      "|         Colorado|        059|           Jefferson|              3.07|\r\n",
      "|         Michigan|        125|             Oakland| 2.888877848101266|\r\n",
      "|         Illinois|        097|                Lake| 2.879328647058823|\r\n",
      "|          Florida|        031|               Duval|2.7794603978494625|\r\n",
      "|    Massachusetts|        017|           Middlesex|2.6500000000000004|\r\n",
      "|           Kansas|        093|              Kearny|2.3753333333333333|\r\n",
      "|     Pennsylvania|        017|               Bucks|2.3674999999999997|\r\n",
      "|       California|        079|     San Luis Obispo|2.3333333333333335|\r\n",
      "|   North Carolina|        065|           Edgecombe|             2.325|\r\n",
      "|           Kansas|        145|              Pawnee|2.2941176470588234|\r\n",
      "|         New York|        119|         Westchester|          2.239375|\r\n",
      "|   North Carolina|        101|            Johnston|             2.225|\r\n",
      "|      Connecticut|        003|            Hartford|2.0787055896226416|\r\n",
      "|   North Carolina|        077|           Granville|2.0285714285714285|\r\n",
      "|       Washington|        003|              Asotin|             2.025|\r\n",
      "|   North Carolina|        061|              Duplin|               2.0|\r\n",
      "|         Colorado|        013|             Boulder| 1.960470901234568|\r\n",
      "|         Arkansas|        035|          Crittenden|1.9000000000000001|\r\n",
      "|   North Carolina|        199|              Yancey|               1.9|\r\n",
      "|       California|        037|         Los Angeles|1.8391350859879247|\r\n",
      "|         Michigan|        163|               Wayne|1.8262007365148463|\r\n",
      "|        Louisiana|        047|           Iberville|1.8213132266666665|\r\n",
      "|   North Carolina|        033|             Caswell|           1.80075|\r\n",
      "|   North Carolina|        037|             Chatham|               1.8|\r\n",
      "|   North Carolina|        147|                Pitt|1.7999999999999998|\r\n",
      "|             Iowa|        045|             Clinton|1.7583018867924527|\r\n",
      "|       California|        025|            Imperial|1.6478600515463917|\r\n",
      "|          Montana|        043|           Jefferson| 1.592159090909091|\r\n",
      "|       California|        033|                Lake| 1.577142857142857|\r\n",
      "|        Wisconsin|        089|             Ozaukee|        1.55733332|\r\n",
      "|          Montana|        095|          Stillwater|1.5384615384615385|\r\n",
      "|        Minnesota|        035|           Crow Wing| 1.532046511627907|\r\n",
      "|        Louisiana|        051|           Jefferson|1.5020496153846155|\r\n",
      "|         Kentucky|        019|                Boyd| 1.485702254901961|\r\n",
      "|       New Jersey|        015|          Gloucester|1.4814285714285715|\r\n",
      "|         Kentucky|        101|           Henderson|1.4295509090909093|\r\n",
      "|          Alabama|        117|              Shelby|1.3927777777777777|\r\n",
      "|            Maine|        011|            Kennebec|             1.375|\r\n",
      "|       California|        099|          Stanislaus|1.2781941269841275|\r\n",
      "|             Iowa|        139|           Muscatine|            1.2375|\r\n",
      "|          Montana|        023|          Deer Lodge|1.2135793388429752|\r\n",
      "|         Colorado|        077|                Mesa|1.2109524924242425|\r\n",
      "|       Washington|        063|             Spokane|1.1469733510638298|\r\n",
      "|        Louisiana|        033|    East Baton Rouge|1.1444728456659612|\r\n",
      "|            Texas|        201|              Harris|1.1426282723128096|\r\n",
      "|             Utah|        011|               Davis|1.1418481240188378|\r\n",
      "|          Montana|        071|            Phillips|1.1414093959731544|\r\n",
      "|         Michigan|        157|             Tuscola|     1.13696140625|\r\n",
      "|            Idaho|        069|           Nez Perce|1.1226492795389047|\r\n",
      "|      Puerto Rico|        021|             Bayamon|1.1200363636363637|\r\n",
      "|         Michigan|        139|              Ottawa|         1.1110375|\r\n",
      "|            Maine|        031|                York|1.0842636320754717|\r\n",
      "|        Minnesota|        117|           Pipestone|1.0807463414634146|\r\n",
      "|           Kansas|        209|           Wyandotte|1.0779660104986872|\r\n",
      "|          Indiana|        089|                Lake|1.0760213851902174|\r\n",
      "|       New Jersey|        017|              Hudson|         1.0754546|\r\n",
      "|            Texas|        491|          Williamson|1.0616129032258064|\r\n",
      "|         Missouri|        510|      St. Louis City|1.0544578440366965|\r\n",
      "|        Minnesota|        173|     Yellow Medicine|1.0511541666666666|\r\n",
      "|       California|        077|         San Joaquin|1.0142639721936149|\r\n",
      "|         Arkansas|        091|              Miller| 1.011219512195122|\r\n",
      "|            Texas|        135|               Ector|1.0014370984405456|\r\n",
      "|           Oregon|        047|              Marion|0.9826868292682926|\r\n",
      "|           Kansas|        181|             Sherman|0.9735323943661971|\r\n",
      "|        Louisiana|        089|         St. Charles|0.9709866896551724|\r\n",
      "|             Ohio|        085|                Lake|0.9666666666666668|\r\n",
      "|        Minnesota|        111|          Otter Tail|0.9641929203539823|\r\n",
      "|       California|        029|                Kern|0.9552840147731252|\r\n",
      "|            Texas|        141|             El Paso|0.9464559802761305|\r\n",
      "|           Kansas|        091|             Johnson|0.9374867924528303|\r\n",
      "|       New Jersey|        021|              Mercer|0.9314939589041096|\r\n",
      "|           Kansas|        029|               Cloud|0.9306044444444446|\r\n",
      "|         Colorado|        035|             Douglas|0.9266666666666667|\r\n",
      "|          Florida|        099|          Palm Beach|0.9141428571428573|\r\n",
      "|          Georgia|        089|              DeKalb|0.9125199496626882|\r\n",
      "|        Minnesota|        061|              Itasca|0.8895128205128204|\r\n",
      "|        Minnesota|        103|            Nicollet| 0.881576923076923|\r\n",
      "|      Connecticut|        001|           Fairfield|0.8765945608108108|\r\n",
      "|    Massachusetts|        013|             Hampden|0.8682349224043717|\r\n",
      "|       California|        019|              Fresno|0.8499170323179771|\r\n",
      "|        Minnesota|        141|           Sherburne|0.8408257575757577|\r\n",
      "|        Wisconsin|        079|           Milwaukee|0.8367436675461741|\r\n",
      "|         Colorado|        045|            Garfield|0.8351016949152542|\r\n",
      "|   South Carolina|        049|             Hampton|0.8288424908424908|\r\n",
      "|           Oregon|        051|           Multnomah|0.8102990317052269|\r\n",
      "|      Mississippi|        059|             Jackson|        0.81023825|\r\n",
      "|      Connecticut|        013|             Tolland|0.8060031923076922|\r\n",
      "|       Washington|        015|             Cowlitz| 0.802640243902439|\r\n",
      "|Country Of Mexico|        002|BAJA CALIFORNIA N...|0.8018910386965376|\r\n",
      "|        Louisiana|        017|               Caddo|             0.798|\r\n",
      "|    Massachusetts|        005|             Bristol|0.7900226190476192|\r\n",
      "+-----------------+-----------+--------------------+------------------+\r\n",
      "only showing top 100 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P2/p2.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P3/spark_sql_p3.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( Year = arr[11][:4], State = arr[24], Arithmetic_mean = float(arr[16])))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    stateRanksDF = spark.sql(\"SELECT Year, \\\n",
    "        State, \\\n",
    "        AVG(Arithmetic_mean) AS Pollutant_levels \\\n",
    "        FROM log \\\n",
    "        GROUP BY State, Year \\\n",
    "        ORDER BY Year, Pollutant_levels\")\n",
    "    stateRanksDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/22 17:40:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 0:==============>                                            (1 + 3) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 2:============================================>              (3 + 1) / 4]',\n",
       " '[Stage 3:===========>                                            (40 + 8) / 200]',\n",
       " '[Stage 3:==================>                                    (66 + 10) / 200]',\n",
       " '[Stage 3:===========================>                           (99 + 10) / 200]',\n",
       " '[Stage 3:===================================>                   (129 + 9) / 200]',\n",
       " '[Stage 3:===========================================>           (159 + 9) / 200]',\n",
       " '[Stage 3:===================================================>   (186 + 8) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m17.998s',\n",
       " 'user\\t0m1.207s',\n",
       " 'sys\\t0m0.933s']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P3/spark_sql_p3.py > ./P3/p3.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\r\n",
      "|Year|               State|    Pollutant_levels|\r\n",
      "+----+--------------------+--------------------+\r\n",
      "|1990|      Virgin Islands|                 0.0|\r\n",
      "|1990|       West Virginia|                 0.0|\r\n",
      "|1990|            Oklahoma|                 0.0|\r\n",
      "|1990|           Wisconsin|                 0.0|\r\n",
      "|1990|              Hawaii|1.970370370370370...|\r\n",
      "|1990|              Nevada|4.208000000000000...|\r\n",
      "|1990|              Alaska|4.420833333333333...|\r\n",
      "|1990|        South Dakota|            5.705E-4|\r\n",
      "|1990|          Washington|5.974999999999999E-4|\r\n",
      "|1990|             Wyoming|6.045454545454545E-4|\r\n",
      "|1990|                Utah|7.970588235294118E-4|\r\n",
      "|1990|          New Mexico|8.222222222222222E-4|\r\n",
      "|1990|              Oregon|8.596296296296297E-4|\r\n",
      "|1990|             Arizona|8.620134228187919E-4|\r\n",
      "|1990|               Maine|9.789285714285713E-4|\r\n",
      "|1990|            Colorado|0.002162374100719...|\r\n",
      "|1990|         Mississippi|0.002666666666666...|\r\n",
      "|1990|            Missouri|              0.0056|\r\n",
      "|1990|            Michigan|0.006559896373056996|\r\n",
      "|1990|         Connecticut|              0.0081|\r\n",
      "|1990|             Georgia|0.008366666666666666|\r\n",
      "|1990|         Puerto Rico|             0.01005|\r\n",
      "|1990|      North Carolina|              0.0143|\r\n",
      "|1990|             Alabama|            0.024325|\r\n",
      "|1990|                Iowa|              0.0332|\r\n",
      "|1990|        Pennsylvania|           0.1059625|\r\n",
      "|1990|                Ohio|            0.135716|\r\n",
      "|1990|            Illinois| 0.14575701219512197|\r\n",
      "|1990|          New Jersey|  0.2933352941176471|\r\n",
      "|1990|           Minnesota|            0.306488|\r\n",
      "|1990|          California| 0.41153099836333895|\r\n",
      "|1990|      South Carolina|  0.5598140350877193|\r\n",
      "|1990|District Of Columbia|  0.8261508196721312|\r\n",
      "|1990|            Virginia|  0.8451416666666666|\r\n",
      "|1990|           Louisiana|  0.9145945945945947|\r\n",
      "|1990|              Kansas|  1.1408154574132492|\r\n",
      "|1990|             Florida|  1.2765626315789476|\r\n",
      "|1990|            New York|   1.362972027972028|\r\n",
      "|1990|               Texas|  1.4824716546762589|\r\n",
      "|1990|             Montana|  2.0686790073529413|\r\n",
      "|1990|       Massachusetts|  3.0246823529411766|\r\n",
      "|1990|             Indiana|   4.098978378378379|\r\n",
      "|1990|           Tennessee|  170.40093066666665|\r\n",
      "|1991|                Ohio|                 0.0|\r\n",
      "|1991|            Arkansas|                 0.0|\r\n",
      "|1991|              Alaska|1.760000000000000...|\r\n",
      "|1991|        South Dakota|2.883333333333333E-4|\r\n",
      "|1991|             Wyoming|            3.258E-4|\r\n",
      "|1991|                Utah|3.378048780487805...|\r\n",
      "|1991|          New Mexico|3.986666666666666...|\r\n",
      "|1991|              Oregon|4.138095238095238E-4|\r\n",
      "|1991|              Nevada|4.826315789473683...|\r\n",
      "|1991|       West Virginia|5.466666666666667E-4|\r\n",
      "|1991|              Hawaii| 6.80754716981132E-4|\r\n",
      "|1991|          Washington|           7.1375E-4|\r\n",
      "|1991|             Georgia|           8.3875E-4|\r\n",
      "|1991|            Kentucky|8.585714285714285E-4|\r\n",
      "|1991|             Arizona|8.828402366863905E-4|\r\n",
      "|1991|      Virgin Islands|9.576470588235293E-4|\r\n",
      "|1991|               Maine|0.001306521739130...|\r\n",
      "|1991|             Vermont|0.001559090909090909|\r\n",
      "|1991|            Colorado| 0.00240126213592233|\r\n",
      "|1991|            Illinois|0.045304637681159415|\r\n",
      "|1991|           Wisconsin|             0.11625|\r\n",
      "|1991|           Minnesota| 0.15858126373626374|\r\n",
      "|1991|             Florida| 0.19555000000000003|\r\n",
      "|1991|        Pennsylvania| 0.21666666666666667|\r\n",
      "|1991|            Virginia|         0.236509875|\r\n",
      "|1991|           Tennessee|  0.2561980769230769|\r\n",
      "|1991|               Texas|   0.280053185840708|\r\n",
      "|1991|            Michigan|  0.3136863049853373|\r\n",
      "|1991|            Maryland| 0.31901639344262295|\r\n",
      "|1991|          California|  0.6735362628865981|\r\n",
      "|1991|District Of Columbia|  0.6772853968253968|\r\n",
      "|1991|             Montana|  0.7670011501597445|\r\n",
      "|1991|      South Carolina|  0.7982056737588653|\r\n",
      "|1991|            New York|  0.8852830188679243|\r\n",
      "|1991|              Kansas|  0.9181956521739129|\r\n",
      "|1991|          New Jersey|  1.0485551612903226|\r\n",
      "|1991|           Louisiana|  1.2000000000000002|\r\n",
      "|1991|             Indiana|  1.6614237288135594|\r\n",
      "|1992|                Ohio|                 0.0|\r\n",
      "|1992|              Alaska|1.026923076923077E-4|\r\n",
      "|1992|              Hawaii|1.894736842105263...|\r\n",
      "|1992|               Idaho|2.343478260869565E-4|\r\n",
      "|1992|          Washington|            2.696E-4|\r\n",
      "|1992|                Utah|2.836538461538461...|\r\n",
      "|1992|          New Mexico|3.153571428571428...|\r\n",
      "|1992|              Nevada|3.333333333333333E-4|\r\n",
      "|1992|             Wyoming|4.655813953488372E-4|\r\n",
      "|1992|            Arkansas|4.702272727272727E-4|\r\n",
      "|1992|             Vermont|4.969565217391304E-4|\r\n",
      "|1992|        South Dakota|4.973913043478262E-4|\r\n",
      "|1992|             Arizona|5.238461538461539E-4|\r\n",
      "|1992|            Virginia|5.474074074074074E-4|\r\n",
      "|1992|               Maine| 6.71111111111111E-4|\r\n",
      "|1992|             Georgia|8.029166666666666E-4|\r\n",
      "|1992|       West Virginia|8.235135135135136E-4|\r\n",
      "|1992|              Oregon|9.852380952380952E-4|\r\n",
      "|1992|            Kentucky|0.001314090909090909|\r\n",
      "+----+--------------------+--------------------+\r\n",
      "only showing top 100 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P3/p3.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P4/spark_sql_p4.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    # files\n",
    "    lines_states = sc.textFile('../data/usa_states.csv')\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    \n",
    "    # file mapping\n",
    "    logRows_states = lines_states.filter( lambda line : len(line) > 0)    \\\n",
    "                    .zipWithIndex() \\\n",
    "                    .filter( lambda x: x[1] > 0) \\\n",
    "                    .map(lambda x: x[0]) \\\n",
    "                    .map( lambda line: line.split(',')) \\\n",
    "                    .map( lambda arr : Row( stateName = arr[1], minLat = arr[2], maxLat = arr[3], \\\n",
    "                                            minLon = arr[4], maxLon = arr[5]))\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr: Row( stateName = arr[24], siteNum = arr[2], countyCode = arr[0]+arr[1], lat = float(arr[5]), lon = float(arr[6]) ) )  \n",
    "    \n",
    "    # Creates the dataframes and views\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    logRows_statesDF = spark.createDataFrame( logRows_states )\n",
    "    logRows_statesDF.createOrReplaceTempView(\"log_states\")\n",
    "    \n",
    "    query = \"SELECT log.name\\\n",
    "        FROM (SELECT log.name,\\\n",
    "            \\\n",
    "        )\"\n",
    "    \n",
    "    query = \"SELECT stateName as State, avg(Dist_Monitor_Center) as Avg_Dist_Monitor_Center \\\n",
    "            FROM (SELECT stateName,\\\n",
    "                countyCode,\\\n",
    "                siteNum,\\\n",
    "                sqrt( pow( (lat-((minLat+maxLat)/2))*111, 2) + pow( (lon-((minLon+maxLon)/2))*111, 2) ) as Dist_Monitor_Center\\\n",
    "                FROM (SELECT log.stateName,\\\n",
    "                    log.countyCode,\\\n",
    "                    log.siteNum,\\\n",
    "                    log.lat,\\\n",
    "                    log.lon,\\\n",
    "                    log_states.minLat,\\\n",
    "                    log_states.maxLat,\\\n",
    "                    log_states.minLon,\\\n",
    "                    log_states.maxLon\\\n",
    "                    FROM log\\\n",
    "                    JOIN log_states ON stateName = name\\\n",
    "                ) table1\\\n",
    "                GROUP BY stateName, countyCode, siteNum, lat, lon, minLat, maxLat, minLon, maxLon\\\n",
    "            ) table2\\\n",
    "            GROUP BY stateName\\\n",
    "            ORDER BY Avg_Dist_Monitor_Center DESC\"\n",
    "\n",
    "    finalDF = spark.sql(query)\n",
    "\n",
    "    \n",
    "    finalDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!!time python ./P4/spark_sql_p4.py > ./P4/p4.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./P4/p4.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P5/spark_sql_p5.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    # files\n",
    "    lines_states = sc.textFile('../data/usa_states.csv')\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    \n",
    "    # file mapping\n",
    "    logRows_states = lines_states.filter( lambda line : len(line) > 0)    \\\n",
    "                    .zipWithIndex() \\\n",
    "                    .filter( lambda x: x[1] > 0) \\\n",
    "                    .map(lambda x: x[0]) \\\n",
    "                    .map( lambda line: line.split(',')) \\\n",
    "                    .map( lambda arr : Row( state = arr[0], name = arr[1], centerLat = (float(arr[2]) + float(arr[3]))/2, \\\n",
    "                                            centerLon = (float(arr[4]) + float(arr[5]))/2))\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr: Row( name = arr[24], countyCode = arr[1], stateNUM = arr[2], lat = float(arr[5]), lon = float(arr[6]) ) )  \n",
    "    \n",
    "    # Creates the dataframes and views\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    logRows_statesDF = spark.createDataFrame( logRows_states )\n",
    "    logRows_statesDF.createOrReplaceTempView(\"log_states\")\n",
    "    \n",
    "    # Atribui a cada monitor único o seu quadrante\n",
    "    MonitorDF = spark.sql(\"SELECT log.name, log.countyCode, log.stateNum, \\\n",
    "     CASE \\\n",
    "         WHEN log.lat > log_states.centerLat AND log.lon > log_states.centerLon THEN 'NE' \\\n",
    "         WHEN log.lat > log_states.centerLat AND log.lon < log_states.centerLon THEN 'SE' \\\n",
    "         WHEN log.lat < log_states.centerLat AND log.lon < log_states.centerLon THEN 'SW' \\\n",
    "         WHEN log.lat < log_states.centerLat AND log.lon > log_states.centerLon THEN 'NW' \\\n",
    "         ELSE 'Center or Borders' \\\n",
    "     END AS Quadrant \\\n",
    "     FROM log JOIN log_states ON log.name=log_states.name GROUP BY log.name, log.countyCode, log.stateNum, Quadrant\")\n",
    "    MonitorDF.createOrReplaceTempView(\"Monitor\")\n",
    "\n",
    "    # Conta o Nr. de monitores em cada quadrante por estado\n",
    "    finalDF = spark.sql(\"SELECT name AS State, Quadrant, count(*) AS Num_Monitors  FROM Monitor GROUP BY name, Quadrant\")\n",
    "    \n",
    "    finalDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/22 17:41:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 2) / 2]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 1:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 1:==============>                                            (1 + 3) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 4:>                  (0 + 2) / 2][Stage 5:>                  (0 + 4) / 4]',\n",
       " '[Stage 4:===================(2 + 0) / 2][Stage 5:>                  (0 + 4) / 4]',\n",
       " '[Stage 5:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 5:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 5:=============================>                             (2 + 2) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 15:==============================>                        (55 + 9) / 100]',\n",
       " '[Stage 15:============================================>          (80 + 8) / 100]',\n",
       " '[Stage 15:==================================================>    (92 + 8) / 100]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m22.162s',\n",
       " 'user\\t0m1.258s',\n",
       " 'sys\\t0m0.905s']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P5/spark_sql_p5.py > ./P5/p5.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------+\r\n",
      "|         State|Quadrant|Num_Monitors|\r\n",
      "+--------------+--------+------------+\r\n",
      "|          Utah|      SE|           6|\r\n",
      "|          Utah|      NW|           3|\r\n",
      "|          Utah|      SW|           3|\r\n",
      "|        Hawaii|      SE|           2|\r\n",
      "|        Hawaii|      NE|           2|\r\n",
      "|        Hawaii|      NW|           1|\r\n",
      "|     Minnesota|      SW|          21|\r\n",
      "|     Minnesota|      NE|          11|\r\n",
      "|     Minnesota|      NW|          50|\r\n",
      "|     Minnesota|      SE|          12|\r\n",
      "|          Ohio|      NE|          30|\r\n",
      "|          Ohio|      NW|          10|\r\n",
      "|          Ohio|      SE|          15|\r\n",
      "|          Ohio|      SW|          34|\r\n",
      "|      Arkansas|      SE|           3|\r\n",
      "|      Arkansas|      SW|           4|\r\n",
      "|      Arkansas|      NE|           2|\r\n",
      "|      Arkansas|      NW|           1|\r\n",
      "|        Oregon|      SW|          12|\r\n",
      "|        Oregon|      NE|           3|\r\n",
      "|        Oregon|      SE|          15|\r\n",
      "|        Oregon|      NW|           1|\r\n",
      "|         Texas|      NW|          72|\r\n",
      "|         Texas|      SE|          23|\r\n",
      "|         Texas|      SW|           3|\r\n",
      "|         Texas|      NE|          34|\r\n",
      "|  North Dakota|      NW|           1|\r\n",
      "|  North Dakota|      SW|           3|\r\n",
      "|  North Dakota|      NE|           1|\r\n",
      "|  North Dakota|      SE|           2|\r\n",
      "|  Pennsylvania|      NW|          35|\r\n",
      "|  Pennsylvania|      SW|          18|\r\n",
      "|  Pennsylvania|      SE|           4|\r\n",
      "|  Pennsylvania|      NE|           3|\r\n",
      "|   Connecticut|      SW|           8|\r\n",
      "|   Connecticut|      SE|           2|\r\n",
      "|   Connecticut|      NE|           5|\r\n",
      "|      Nebraska|      NW|           3|\r\n",
      "|      Nebraska|      SE|           2|\r\n",
      "|      Nebraska|      NE|           1|\r\n",
      "|       Vermont|      SE|          16|\r\n",
      "|       Vermont|      SW|           4|\r\n",
      "|       Vermont|      NW|           1|\r\n",
      "|       Vermont|      NE|           1|\r\n",
      "|        Nevada|      NE|           2|\r\n",
      "|        Nevada|      SE|           2|\r\n",
      "|        Nevada|      NW|           4|\r\n",
      "|   Puerto Rico|      NE|           4|\r\n",
      "|   Puerto Rico|      SE|           1|\r\n",
      "|   Puerto Rico|      SW|           1|\r\n",
      "|    Washington|      SE|          20|\r\n",
      "|    Washington|      NE|           6|\r\n",
      "|    Washington|      SW|          14|\r\n",
      "|    Washington|      NW|           2|\r\n",
      "|      Illinois|      NE|          32|\r\n",
      "|      Illinois|      SW|          15|\r\n",
      "|      Illinois|      SE|           2|\r\n",
      "|      Illinois|      NW|           1|\r\n",
      "|      Oklahoma|      NE|          17|\r\n",
      "|      Oklahoma|      NW|           2|\r\n",
      "|      Oklahoma|      SE|           1|\r\n",
      "|Virgin Islands|      SW|           4|\r\n",
      "|      Delaware|      SW|           2|\r\n",
      "|      Delaware|      SE|           4|\r\n",
      "|        Alaska|      NE|           4|\r\n",
      "|        Alaska|      SW|           3|\r\n",
      "|        Alaska|      SE|           3|\r\n",
      "|        Alaska|      NW|           2|\r\n",
      "|    New Mexico|      SE|          11|\r\n",
      "|    New Mexico|      NE|           1|\r\n",
      "|    New Mexico|      NW|           2|\r\n",
      "|    New Mexico|      SW|           3|\r\n",
      "| West Virginia|      SW|           5|\r\n",
      "| West Virginia|      NE|           2|\r\n",
      "| West Virginia|      SE|           3|\r\n",
      "|      Missouri|      NE|           8|\r\n",
      "|      Missouri|      SW|           2|\r\n",
      "|      Missouri|      SE|           3|\r\n",
      "|      Missouri|      NW|           2|\r\n",
      "|  Rhode Island|      NE|          11|\r\n",
      "|  Rhode Island|      SE|           1|\r\n",
      "|       Georgia|      SE|          20|\r\n",
      "|       Georgia|      NW|           5|\r\n",
      "|       Georgia|      SW|           5|\r\n",
      "|       Georgia|      NE|           4|\r\n",
      "|       Montana|      SE|           9|\r\n",
      "|       Montana|      SW|          36|\r\n",
      "|       Montana|      NE|          11|\r\n",
      "|       Montana|      NW|           4|\r\n",
      "|      Michigan|      NW|          69|\r\n",
      "|      Michigan|      NE|          11|\r\n",
      "|      Michigan|      SE|           3|\r\n",
      "|      Michigan|      SW|           1|\r\n",
      "|      Virginia|      NW|           8|\r\n",
      "|      Virginia|      NE|           8|\r\n",
      "|      Virginia|      SW|           4|\r\n",
      "|North Carolina|      SE|          18|\r\n",
      "|North Carolina|      NE|          26|\r\n",
      "|North Carolina|      NW|           5|\r\n",
      "|       Wyoming|      SW|           2|\r\n",
      "+--------------+--------+------------+\r\n",
      "only showing top 100 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P5/p5.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
