{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Spark SQL Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all scripts executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"chmod: changing permissions of './P1/spark_sql_p1.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P2/spark_sql_p2.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P3/spark_sql_p3.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P4/spark_sql_p4.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P5/spark_sql_p5.py': Operation not permitted\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!chmod a+x ./*/*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./*/*.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P1/spark_sql_p1.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( state_name = arr[24], site_num = arr[2], state_code = arr[0], county_code = arr[1]))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    query = query = \"SELECT state_name, COUNT(*) AS number_monitors FROM (SELECT DISTINCT * FROM log) GROUP BY state_name ORDER BY number_monitors DESC\"\n",
    "\n",
    "    final = spark.sql(query)\n",
    "\n",
    "    final.show(30)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:13:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 3:=======>                                                (27 + 9) / 200]',\n",
       " '[Stage 3:=============>                                          (47 + 8) / 200]',\n",
       " '[Stage 3:===================>                                    (70 + 8) / 200]',\n",
       " '[Stage 3:==========================>                             (93 + 8) / 200]',\n",
       " '[Stage 3:================================>                      (119 + 9) / 200]',\n",
       " '[Stage 3:=======================================>               (143 + 8) / 200]',\n",
       " '[Stage 3:===============================================>       (172 + 8) / 200]',\n",
       " '[Stage 4:==============================================>       (172 + 10) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m16.963s',\n",
       " 'user\\t0m1.172s',\n",
       " 'sys\\t0m0.907s']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P1/spark_sql_p1.py > ./P1/p1.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\r\n",
      "|    state_name|number_monitors|\r\n",
      "+--------------+---------------+\r\n",
      "|    California|            162|\r\n",
      "|         Texas|            132|\r\n",
      "|     Minnesota|             94|\r\n",
      "|          Ohio|             89|\r\n",
      "|      Michigan|             84|\r\n",
      "|      New York|             66|\r\n",
      "|South Carolina|             64|\r\n",
      "|  Pennsylvania|             60|\r\n",
      "|       Montana|             60|\r\n",
      "|       Indiana|             52|\r\n",
      "|      Colorado|             51|\r\n",
      "|       Florida|             50|\r\n",
      "|      Illinois|             50|\r\n",
      "|North Carolina|             49|\r\n",
      "|    Washington|             42|\r\n",
      "|     Louisiana|             40|\r\n",
      "|       Arizona|             38|\r\n",
      "|        Kansas|             37|\r\n",
      "|       Georgia|             34|\r\n",
      "|        Oregon|             31|\r\n",
      "|      Kentucky|             30|\r\n",
      "|       Alabama|             28|\r\n",
      "|     Tennessee|             27|\r\n",
      "|    New Jersey|             24|\r\n",
      "|     Wisconsin|             24|\r\n",
      "|       Vermont|             22|\r\n",
      "|   Mississippi|             21|\r\n",
      "|         Maine|             21|\r\n",
      "|      Virginia|             20|\r\n",
      "|      Oklahoma|             20|\r\n",
      "+--------------+---------------+\r\n",
      "only showing top 30 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P1/p1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P2/spark_sql_p2.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( State = arr[24], County = arr[25], countyCode = arr[1], Arithmetic_mean = float(arr[16])))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    stateRanksDF = spark.sql(\"SELECT County, \\\n",
    "        AVG(Arithmetic_mean) AS Pollutant_levels \\\n",
    "        FROM log GROUP BY State, countyCode, County \\\n",
    "        ORDER BY Pollutant_levels DESC\")\n",
    "    stateRanksDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:13:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 0:=============================>                             (2 + 2) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 2:=============================>                             (2 + 2) / 4]',\n",
       " '[Stage 3:============================>                          (102 + 8) / 200]',\n",
       " '[Stage 3:=====================================>                (138 + 14) / 200]',\n",
       " '[Stage 3:================================================>     (181 + 11) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m16.632s',\n",
       " 'user\\t0m1.307s',\n",
       " 'sys\\t0m1.010s']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P2/spark_sql_p2.py > ./P2/p2.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\r\n",
      "|              County|  Pollutant_levels|\r\n",
      "+--------------------+------------------+\r\n",
      "|              Tipton|            2556.0|\r\n",
      "|              Nassau|              19.0|\r\n",
      "|          Columbiana| 7.385690735785953|\r\n",
      "|                Park| 5.611212121212121|\r\n",
      "|     CHIHUAHUA STATE|         4.5121875|\r\n",
      "|            Caldwell| 4.116666666666667|\r\n",
      "|               Kings|3.9843770491803276|\r\n",
      "|              Madera|            3.7393|\r\n",
      "|            Franklin|3.3499999999999996|\r\n",
      "|           Jefferson|              3.07|\r\n",
      "|             Oakland| 2.888877848101266|\r\n",
      "|                Lake| 2.879328647058823|\r\n",
      "|               Duval|2.7794603978494625|\r\n",
      "|           Middlesex|2.6500000000000004|\r\n",
      "|              Kearny|2.3753333333333333|\r\n",
      "|               Bucks|2.3674999999999997|\r\n",
      "|     San Luis Obispo|2.3333333333333335|\r\n",
      "|           Edgecombe|             2.325|\r\n",
      "|              Pawnee|2.2941176470588234|\r\n",
      "|         Westchester|          2.239375|\r\n",
      "|            Johnston|             2.225|\r\n",
      "|            Hartford|2.0787055896226416|\r\n",
      "|           Granville|2.0285714285714285|\r\n",
      "|              Asotin|             2.025|\r\n",
      "|              Duplin|               2.0|\r\n",
      "|             Boulder| 1.960470901234568|\r\n",
      "|          Crittenden|1.9000000000000001|\r\n",
      "|              Yancey|               1.9|\r\n",
      "|         Los Angeles|1.8391350859879247|\r\n",
      "|               Wayne|1.8262007365148463|\r\n",
      "|           Iberville|1.8213132266666665|\r\n",
      "|             Caswell|           1.80075|\r\n",
      "|             Chatham|               1.8|\r\n",
      "|                Pitt|1.7999999999999998|\r\n",
      "|             Clinton|1.7583018867924527|\r\n",
      "|            Imperial|1.6478600515463917|\r\n",
      "|           Jefferson| 1.592159090909091|\r\n",
      "|                Lake| 1.577142857142857|\r\n",
      "|             Ozaukee|        1.55733332|\r\n",
      "|          Stillwater|1.5384615384615385|\r\n",
      "|           Crow Wing| 1.532046511627907|\r\n",
      "|           Jefferson|1.5020496153846155|\r\n",
      "|                Boyd| 1.485702254901961|\r\n",
      "|          Gloucester|1.4814285714285715|\r\n",
      "|           Henderson|1.4295509090909093|\r\n",
      "|              Shelby|1.3927777777777777|\r\n",
      "|            Kennebec|             1.375|\r\n",
      "|          Stanislaus|1.2781941269841275|\r\n",
      "|           Muscatine|            1.2375|\r\n",
      "|          Deer Lodge|1.2135793388429752|\r\n",
      "|                Mesa|1.2109524924242425|\r\n",
      "|             Spokane|1.1469733510638298|\r\n",
      "|    East Baton Rouge|1.1444728456659612|\r\n",
      "|              Harris|1.1426282723128096|\r\n",
      "|               Davis|1.1418481240188378|\r\n",
      "|            Phillips|1.1414093959731544|\r\n",
      "|             Tuscola|     1.13696140625|\r\n",
      "|           Nez Perce|1.1226492795389047|\r\n",
      "|             Bayamon|1.1200363636363637|\r\n",
      "|              Ottawa|         1.1110375|\r\n",
      "|                York|1.0842636320754717|\r\n",
      "|           Pipestone|1.0807463414634146|\r\n",
      "|           Wyandotte|1.0779660104986872|\r\n",
      "|                Lake|1.0760213851902174|\r\n",
      "|              Hudson|         1.0754546|\r\n",
      "|          Williamson|1.0616129032258064|\r\n",
      "|      St. Louis City|1.0544578440366965|\r\n",
      "|     Yellow Medicine|1.0511541666666666|\r\n",
      "|         San Joaquin|1.0142639721936149|\r\n",
      "|              Miller| 1.011219512195122|\r\n",
      "|               Ector|1.0014370984405456|\r\n",
      "|              Marion|0.9826868292682926|\r\n",
      "|             Sherman|0.9735323943661971|\r\n",
      "|         St. Charles|0.9709866896551724|\r\n",
      "|                Lake|0.9666666666666668|\r\n",
      "|          Otter Tail|0.9641929203539823|\r\n",
      "|                Kern|0.9552840147731252|\r\n",
      "|             El Paso|0.9464559802761305|\r\n",
      "|             Johnson|0.9374867924528303|\r\n",
      "|              Mercer|0.9314939589041096|\r\n",
      "|               Cloud|0.9306044444444446|\r\n",
      "|             Douglas|0.9266666666666667|\r\n",
      "|          Palm Beach|0.9141428571428573|\r\n",
      "|              DeKalb|0.9125199496626882|\r\n",
      "|              Itasca|0.8895128205128204|\r\n",
      "|            Nicollet| 0.881576923076923|\r\n",
      "|           Fairfield|0.8765945608108108|\r\n",
      "|             Hampden|0.8682349224043717|\r\n",
      "|              Fresno|0.8499170323179771|\r\n",
      "|           Sherburne|0.8408257575757577|\r\n",
      "|           Milwaukee|0.8367436675461741|\r\n",
      "|            Garfield|0.8351016949152542|\r\n",
      "|             Hampton|0.8288424908424908|\r\n",
      "|           Multnomah|0.8102990317052269|\r\n",
      "|             Jackson|        0.81023825|\r\n",
      "|             Tolland|0.8060031923076922|\r\n",
      "|             Cowlitz| 0.802640243902439|\r\n",
      "|BAJA CALIFORNIA N...|0.8018910386965376|\r\n",
      "|               Caddo|             0.798|\r\n",
      "|             Bristol|0.7900226190476192|\r\n",
      "+--------------------+------------------+\r\n",
      "only showing top 100 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P2/p2.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P3/spark_sql_p3.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( Year = arr[11][:4], State = arr[24], Arithmetic_mean = float(arr[16])))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    stateRanksDF = spark.sql(\"SELECT Year, \\\n",
    "        State, \\\n",
    "        AVG(Arithmetic_mean) AS Pollutant_levels \\\n",
    "        FROM log \\\n",
    "        GROUP BY State, Year \\\n",
    "        ORDER BY Year, Pollutant_levels\")\n",
    "    stateRanksDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:14:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 2:============================================>              (3 + 1) / 4]',\n",
       " '[Stage 3:====================>                                   (74 + 8) / 200]',\n",
       " '[Stage 3:==============================>                        (111 + 9) / 200]',\n",
       " '[Stage 3:======================================>               (142 + 11) / 200]',\n",
       " '[Stage 3:==================================================>    (184 + 8) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m17.521s',\n",
       " 'user\\t0m1.313s',\n",
       " 'sys\\t0m0.904s']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P3/spark_sql_p3.py > ./P3/p3.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\r\n",
      "|Year|               State|    Pollutant_levels|\r\n",
      "+----+--------------------+--------------------+\r\n",
      "|1990|           Wisconsin|                 0.0|\r\n",
      "|1990|            Oklahoma|                 0.0|\r\n",
      "|1990|      Virgin Islands|                 0.0|\r\n",
      "|1990|       West Virginia|                 0.0|\r\n",
      "|1990|              Hawaii|1.970370370370370...|\r\n",
      "|1990|              Nevada|4.208000000000000...|\r\n",
      "|1990|              Alaska|4.420833333333333...|\r\n",
      "|1990|        South Dakota|            5.705E-4|\r\n",
      "|1990|          Washington|5.974999999999999E-4|\r\n",
      "|1990|             Wyoming|6.045454545454545E-4|\r\n",
      "|1990|                Utah|7.970588235294118E-4|\r\n",
      "|1990|          New Mexico|8.222222222222222E-4|\r\n",
      "|1990|              Oregon|8.596296296296297E-4|\r\n",
      "|1990|             Arizona|8.620134228187919E-4|\r\n",
      "|1990|               Maine|9.789285714285713E-4|\r\n",
      "|1990|            Colorado|0.002162374100719...|\r\n",
      "|1990|         Mississippi|0.002666666666666...|\r\n",
      "|1990|            Missouri|              0.0056|\r\n",
      "|1990|            Michigan|0.006559896373056996|\r\n",
      "|1990|         Connecticut|              0.0081|\r\n",
      "|1990|             Georgia|0.008366666666666666|\r\n",
      "|1990|         Puerto Rico|             0.01005|\r\n",
      "|1990|      North Carolina|              0.0143|\r\n",
      "|1990|             Alabama|            0.024325|\r\n",
      "|1990|                Iowa|              0.0332|\r\n",
      "|1990|        Pennsylvania|           0.1059625|\r\n",
      "|1990|                Ohio|            0.135716|\r\n",
      "|1990|            Illinois| 0.14575701219512197|\r\n",
      "|1990|          New Jersey|  0.2933352941176471|\r\n",
      "|1990|           Minnesota|            0.306488|\r\n",
      "|1990|          California| 0.41153099836333895|\r\n",
      "|1990|      South Carolina|  0.5598140350877193|\r\n",
      "|1990|District Of Columbia|  0.8261508196721312|\r\n",
      "|1990|            Virginia|  0.8451416666666666|\r\n",
      "|1990|           Louisiana|  0.9145945945945947|\r\n",
      "|1990|              Kansas|  1.1408154574132492|\r\n",
      "|1990|             Florida|  1.2765626315789476|\r\n",
      "|1990|            New York|   1.362972027972028|\r\n",
      "|1990|               Texas|  1.4824716546762589|\r\n",
      "|1990|             Montana|  2.0686790073529413|\r\n",
      "|1990|       Massachusetts|  3.0246823529411766|\r\n",
      "|1990|             Indiana|   4.098978378378379|\r\n",
      "|1990|           Tennessee|  170.40093066666665|\r\n",
      "|1991|                Ohio|                 0.0|\r\n",
      "|1991|            Arkansas|                 0.0|\r\n",
      "|1991|              Alaska|1.760000000000000...|\r\n",
      "|1991|        South Dakota|2.883333333333333E-4|\r\n",
      "|1991|             Wyoming|            3.258E-4|\r\n",
      "|1991|                Utah|3.378048780487805...|\r\n",
      "|1991|          New Mexico|3.986666666666666...|\r\n",
      "|1991|              Oregon|4.138095238095238E-4|\r\n",
      "|1991|              Nevada|4.826315789473683...|\r\n",
      "|1991|       West Virginia|5.466666666666667E-4|\r\n",
      "|1991|              Hawaii| 6.80754716981132E-4|\r\n",
      "|1991|          Washington|           7.1375E-4|\r\n",
      "|1991|             Georgia|           8.3875E-4|\r\n",
      "|1991|            Kentucky|8.585714285714285E-4|\r\n",
      "|1991|             Arizona|8.828402366863905E-4|\r\n",
      "|1991|      Virgin Islands|9.576470588235293E-4|\r\n",
      "|1991|               Maine|0.001306521739130...|\r\n",
      "|1991|             Vermont|0.001559090909090909|\r\n",
      "|1991|            Colorado| 0.00240126213592233|\r\n",
      "|1991|            Illinois|0.045304637681159415|\r\n",
      "|1991|           Wisconsin|             0.11625|\r\n",
      "|1991|           Minnesota| 0.15858126373626374|\r\n",
      "|1991|             Florida| 0.19555000000000003|\r\n",
      "|1991|        Pennsylvania| 0.21666666666666667|\r\n",
      "|1991|            Virginia|         0.236509875|\r\n",
      "|1991|           Tennessee|  0.2561980769230769|\r\n",
      "|1991|               Texas|   0.280053185840708|\r\n",
      "|1991|            Michigan|  0.3136863049853373|\r\n",
      "|1991|            Maryland| 0.31901639344262295|\r\n",
      "|1991|          California|  0.6735362628865981|\r\n",
      "|1991|District Of Columbia|  0.6772853968253968|\r\n",
      "|1991|             Montana|  0.7670011501597445|\r\n",
      "|1991|      South Carolina|  0.7982056737588653|\r\n",
      "|1991|            New York|  0.8852830188679243|\r\n",
      "|1991|              Kansas|  0.9181956521739129|\r\n",
      "|1991|          New Jersey|  1.0485551612903226|\r\n",
      "|1991|           Louisiana|  1.2000000000000002|\r\n",
      "|1991|             Indiana|  1.6614237288135594|\r\n",
      "|1992|                Ohio|                 0.0|\r\n",
      "|1992|              Alaska|1.026923076923077E-4|\r\n",
      "|1992|              Hawaii|1.894736842105263...|\r\n",
      "|1992|               Idaho|2.343478260869565E-4|\r\n",
      "|1992|          Washington|            2.696E-4|\r\n",
      "|1992|                Utah|2.836538461538461...|\r\n",
      "|1992|          New Mexico|3.153571428571428...|\r\n",
      "|1992|              Nevada|3.333333333333333E-4|\r\n",
      "|1992|             Wyoming|4.655813953488372E-4|\r\n",
      "|1992|            Arkansas|4.702272727272727E-4|\r\n",
      "|1992|             Vermont|4.969565217391304E-4|\r\n",
      "|1992|        South Dakota|4.973913043478262E-4|\r\n",
      "|1992|             Arizona|5.238461538461539E-4|\r\n",
      "|1992|            Virginia|5.474074074074074E-4|\r\n",
      "|1992|               Maine| 6.71111111111111E-4|\r\n",
      "|1992|             Georgia|8.029166666666666E-4|\r\n",
      "|1992|       West Virginia|8.235135135135136E-4|\r\n",
      "|1992|              Oregon|9.852380952380952E-4|\r\n",
      "|1992|            Kentucky|0.001314090909090909|\r\n",
      "+----+--------------------+--------------------+\r\n",
      "only showing top 100 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P3/p3.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P4/spark_sql_p4.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    # files\n",
    "    lines_states = sc.textFile('../data/usa_states.csv')\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    \n",
    "    # file mapping\n",
    "    logRows_states = lines_states.filter( lambda line : len(line) > 0)    \\\n",
    "                    .zipWithIndex() \\\n",
    "                    .filter( lambda x: x[1] > 0) \\\n",
    "                    .map(lambda x: x[0]) \\\n",
    "                    .map( lambda line: line.split(',')) \\\n",
    "                    .map( lambda arr : Row( name = arr[1], centerLat = (float(arr[2])+float(arr[3]))/2, \\\n",
    "                                            centerLon = (float(arr[4])+float(arr[5]))/2))\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr: Row( stateName = arr[24], siteNum = arr[2], countyCode = arr[1], lat = float(\"{:.3f}\".format(float(arr[5]))), lon = float(\"{:.3f}\".format(float(arr[6]))) ) )  \n",
    "    \n",
    "    # Creates the dataframes and views\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    logRows_statesDF = spark.createDataFrame( logRows_states )\n",
    "    logRows_statesDF.createOrReplaceTempView(\"log_states\")\n",
    "\n",
    "    query = \"SELECT stateName as State, AVG(Dist_Monitor_Center) as Avg_Dist_Monitor_Center \\\n",
    "    FROM ( SELECT stateName, \\\n",
    "        sqrt( pow( (AVG(lat-centerLat))*111, 2) + pow( (AVG(lon-centerLon))*111, 2) ) as Dist_Monitor_Center \\\n",
    "        FROM log JOIN log_states ON stateName = name\\\n",
    "        GROUP BY stateName, countyCode, siteNum )\\\n",
    "    GROUP BY State\"\n",
    "\n",
    "    finalDF = spark.sql(query)\n",
    "\n",
    "    \n",
    "    finalDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 2) / 2]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 1:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 1:==============>                                            (1 + 3) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 4:>                  (0 + 4) / 4][Stage 5:>                  (0 + 2) / 2]',\n",
       " '[Stage 4:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 4:==============>                                            (1 + 3) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 15:=======================>                               (42 + 8) / 100]',\n",
       " '[Stage 15:================================>                      (59 + 9) / 100]',\n",
       " '[Stage 15:===========================================>           (79 + 8) / 100]',\n",
       " '[Stage 15:==================================================>    (91 + 9) / 100]',\n",
       " '[Stage 15:===================================================>   (93 + 7) / 100]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 18:=========================================>              (56 + 8) / 75]',\n",
       " '[Stage 18:======================================================> (73 + 2) / 75]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m22.304s',\n",
       " 'user\\t0m1.329s',\n",
       " 'sys\\t0m0.908s']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P4/spark_sql_p4.py > ./P4/p4.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+\r\n",
      "|         State|Avg_Dist_Monitor_Center|\r\n",
      "+--------------+-----------------------+\r\n",
      "|          Utah|     184.91876919510136|\r\n",
      "|        Hawaii|     155.72848584906387|\r\n",
      "|     Minnesota|     195.06726533715846|\r\n",
      "|          Ohio|     175.74265339396345|\r\n",
      "|      Arkansas|     151.13713852810923|\r\n",
      "|        Oregon|      270.4530697644639|\r\n",
      "|         Texas|      512.0338063321824|\r\n",
      "|  North Dakota|     248.43249153972548|\r\n",
      "|  Pennsylvania|     250.65578227993026|\r\n",
      "|   Connecticut|      49.99224954412992|\r\n",
      "|      Nebraska|     307.13572198974225|\r\n",
      "|       Vermont|      521.9872635614976|\r\n",
      "|        Nevada|     325.85331502151854|\r\n",
      "|   Puerto Rico|     32.733405599511656|\r\n",
      "|    Washington|     223.06324162734512|\r\n",
      "|      Illinois|     435.24508277080764|\r\n",
      "|      Oklahoma|     236.71168497753382|\r\n",
      "|Virgin Islands|      78.42453456606204|\r\n",
      "|      Delaware|      51.57775481518227|\r\n",
      "|        Alaska|      603.7055510312542|\r\n",
      "|    New Mexico|     183.10481660851678|\r\n",
      "| West Virginia|      144.4948207089263|\r\n",
      "|      Missouri|     228.70294685662293|\r\n",
      "|  Rhode Island|     21.559466890271334|\r\n",
      "|       Georgia|     185.57120999020063|\r\n",
      "|       Montana|     282.35183933196845|\r\n",
      "|      Michigan|      329.4440408970378|\r\n",
      "|      Virginia|     1167.8266779336666|\r\n",
      "|North Carolina|      180.4816046933582|\r\n",
      "|       Wyoming|      283.6508345765912|\r\n",
      "|        Kansas|     292.07813292415466|\r\n",
      "|    New Jersey|      80.74117031360078|\r\n",
      "|      Maryland|      89.29990092593128|\r\n",
      "|       Alabama|      168.6858167460084|\r\n",
      "|       Arizona|     178.89012459120076|\r\n",
      "|          Iowa|      206.5833608417489|\r\n",
      "| Massachusetts|      92.34794730969065|\r\n",
      "|      Kentucky|      219.3113968562989|\r\n",
      "|     Louisiana|     178.22826309633496|\r\n",
      "|   Mississippi|     174.23332103586188|\r\n",
      "| New Hampshire|      116.0111854134792|\r\n",
      "|     Tennessee|      243.8564917630969|\r\n",
      "|       Florida|     333.77014998726025|\r\n",
      "|       Indiana|     177.74179566801655|\r\n",
      "|         Idaho|       289.628874598076|\r\n",
      "|South Carolina|     131.48946099874937|\r\n",
      "|  South Dakota|      365.8476206180499|\r\n",
      "|    California|      328.1742590897139|\r\n",
      "|      New York|      283.5204283804173|\r\n",
      "|     Wisconsin|     206.85563757355047|\r\n",
      "|      Colorado|      180.2550883899222|\r\n",
      "|         Maine|     167.76358421309274|\r\n",
      "+--------------+-----------------------+\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P4/p4.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./P5/spark_sql_p5.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    # files\n",
    "    lines_states = sc.textFile('../data/usa_states.csv')\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    \n",
    "    # file mapping\n",
    "    logRows_states = lines_states.filter( lambda line : len(line) > 0)    \\\n",
    "                    .zipWithIndex() \\\n",
    "                    .filter( lambda x: x[1] > 0) \\\n",
    "                    .map(lambda x: x[0]) \\\n",
    "                    .map( lambda line: line.split(',')) \\\n",
    "                    .map( lambda arr : Row( state = arr[0], name = arr[1], centerLat = (float(arr[2]) + float(arr[3]))/2, \\\n",
    "                                            centerLon = (float(arr[4]) + float(arr[5]))/2))\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr: Row( name = arr[24], countyCode = arr[1], stateNUM = arr[2], lat = float(\"{:.3f}\".format(float(arr[5]))), lon = float(\"{:.3f}\".format(float(arr[6]))) ) )  \n",
    "    \n",
    "    # Creates the dataframes and views\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    logRows_statesDF = spark.createDataFrame( logRows_states )\n",
    "    logRows_statesDF.createOrReplaceTempView(\"log_states\")\n",
    "    \n",
    "    # Atribui a cada monitor Ãºnico o seu quadrante\n",
    "    MonitorDF = spark.sql(\"SELECT log.name, log.countyCode, log.stateNum, \\\n",
    "     CASE \\\n",
    "         WHEN log.lat < log_states.centerLat AND log.lon > log_states.centerLon THEN 'NE' \\\n",
    "         WHEN log.lat > log_states.centerLat AND log.lon > log_states.centerLon THEN 'SE' \\\n",
    "         WHEN log.lat > log_states.centerLat AND log.lon < log_states.centerLon THEN 'SW' \\\n",
    "         WHEN log.lat < log_states.centerLat AND log.lon < log_states.centerLon THEN 'NW' \\\n",
    "         ELSE 'Center or Borders' \\\n",
    "     END AS Quadrant \\\n",
    "     FROM log JOIN log_states ON log.name=log_states.name GROUP BY log.name, log.countyCode, log.stateNum, Quadrant\")\n",
    "    MonitorDF.createOrReplaceTempView(\"Monitor\")\n",
    "\n",
    "    # Conta o Nr. de monitores em cada quadrante por estado\n",
    "    finalDF = spark.sql(\"SELECT name AS State, Quadrant, count(*) AS Num_Monitors  FROM Monitor GROUP BY name, Quadrant\")\n",
    "    \n",
    "    finalDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:15:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 2) / 2]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 1:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 1:==============>                                            (1 + 3) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 4:>                  (0 + 2) / 2][Stage 5:>                  (0 + 4) / 4]',\n",
       " '[Stage 5:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 5:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 5:=============================>                             (2 + 2) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 15:=================================>                     (60 + 8) / 100]',\n",
       " '[Stage 15:===========================================>           (79 + 8) / 100]',\n",
       " '[Stage 15:=================================================>     (90 + 8) / 100]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m22.528s',\n",
       " 'user\\t0m1.586s',\n",
       " 'sys\\t0m0.928s']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P5/spark_sql_p5.py > ./P5/p5.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------+\r\n",
      "|         State|Quadrant|Num_Monitors|\r\n",
      "+--------------+--------+------------+\r\n",
      "|          Utah|      SW|           6|\r\n",
      "|          Utah|      NE|           3|\r\n",
      "|          Utah|      NW|           3|\r\n",
      "|        Hawaii|      SW|           2|\r\n",
      "|        Hawaii|      SE|           2|\r\n",
      "|        Hawaii|      NE|           1|\r\n",
      "|     Minnesota|      NW|          21|\r\n",
      "|     Minnesota|      SE|          11|\r\n",
      "|     Minnesota|      NE|          50|\r\n",
      "|     Minnesota|      SW|          12|\r\n",
      "|          Ohio|      SE|          30|\r\n",
      "|          Ohio|      NE|          10|\r\n",
      "|          Ohio|      SW|          15|\r\n",
      "|          Ohio|      NW|          34|\r\n",
      "|      Arkansas|      SW|           3|\r\n",
      "|      Arkansas|      NW|           4|\r\n",
      "|      Arkansas|      SE|           2|\r\n",
      "|      Arkansas|      NE|           1|\r\n",
      "|        Oregon|      NW|          12|\r\n",
      "|        Oregon|      SE|           3|\r\n",
      "|        Oregon|      SW|          15|\r\n",
      "|        Oregon|      NE|           1|\r\n",
      "|         Texas|      NE|          72|\r\n",
      "|         Texas|      SW|          23|\r\n",
      "|         Texas|      NW|           3|\r\n",
      "|         Texas|      SE|          34|\r\n",
      "|  North Dakota|      NE|           1|\r\n",
      "|  North Dakota|      NW|           3|\r\n",
      "|  North Dakota|      SE|           1|\r\n",
      "|  North Dakota|      SW|           2|\r\n",
      "|  Pennsylvania|      NE|          35|\r\n",
      "|  Pennsylvania|      NW|          18|\r\n",
      "|  Pennsylvania|      SW|           4|\r\n",
      "|  Pennsylvania|      SE|           3|\r\n",
      "|   Connecticut|      NW|           8|\r\n",
      "|   Connecticut|      SW|           2|\r\n",
      "|   Connecticut|      SE|           5|\r\n",
      "|      Nebraska|      NE|           3|\r\n",
      "|      Nebraska|      SW|           2|\r\n",
      "|      Nebraska|      SE|           1|\r\n",
      "|       Vermont|      SW|          16|\r\n",
      "|       Vermont|      NW|           4|\r\n",
      "|       Vermont|      NE|           1|\r\n",
      "|       Vermont|      SE|           1|\r\n",
      "|        Nevada|      SE|           2|\r\n",
      "|        Nevada|      SW|           2|\r\n",
      "|        Nevada|      NE|           4|\r\n",
      "|   Puerto Rico|      SE|           4|\r\n",
      "|   Puerto Rico|      SW|           1|\r\n",
      "|   Puerto Rico|      NW|           1|\r\n",
      "|    Washington|      SW|          20|\r\n",
      "|    Washington|      SE|           6|\r\n",
      "|    Washington|      NW|          14|\r\n",
      "|    Washington|      NE|           2|\r\n",
      "|      Illinois|      SE|          32|\r\n",
      "|      Illinois|      NW|          15|\r\n",
      "|      Illinois|      SW|           2|\r\n",
      "|      Illinois|      NE|           1|\r\n",
      "|      Oklahoma|      SE|          17|\r\n",
      "|      Oklahoma|      NE|           2|\r\n",
      "|      Oklahoma|      SW|           1|\r\n",
      "|Virgin Islands|      NW|           4|\r\n",
      "|      Delaware|      NW|           2|\r\n",
      "|      Delaware|      SW|           4|\r\n",
      "|        Alaska|      SE|           4|\r\n",
      "|        Alaska|      NW|           3|\r\n",
      "|        Alaska|      SW|           3|\r\n",
      "|        Alaska|      NE|           2|\r\n",
      "|    New Mexico|      SW|          11|\r\n",
      "|    New Mexico|      SE|           1|\r\n",
      "|    New Mexico|      NE|           2|\r\n",
      "|    New Mexico|      NW|           3|\r\n",
      "| West Virginia|      NW|           5|\r\n",
      "| West Virginia|      SE|           2|\r\n",
      "| West Virginia|      SW|           3|\r\n",
      "|      Missouri|      SE|           8|\r\n",
      "|      Missouri|      NW|           2|\r\n",
      "|      Missouri|      SW|           3|\r\n",
      "|      Missouri|      NE|           2|\r\n",
      "|  Rhode Island|      SE|          11|\r\n",
      "|  Rhode Island|      SW|           1|\r\n",
      "|       Georgia|      SW|          20|\r\n",
      "|       Georgia|      NE|           5|\r\n",
      "|       Georgia|      NW|           5|\r\n",
      "|       Georgia|      SE|           4|\r\n",
      "|       Montana|      SW|           9|\r\n",
      "|       Montana|      NW|          36|\r\n",
      "|       Montana|      SE|          11|\r\n",
      "|       Montana|      NE|           4|\r\n",
      "|      Michigan|      NE|          69|\r\n",
      "|      Michigan|      SE|          11|\r\n",
      "|      Michigan|      SW|           3|\r\n",
      "|      Michigan|      NW|           1|\r\n",
      "|      Virginia|      NE|           8|\r\n",
      "|      Virginia|      SE|           8|\r\n",
      "|      Virginia|      NW|           4|\r\n",
      "|North Carolina|      SW|          18|\r\n",
      "|North Carolina|      SE|          26|\r\n",
      "|North Carolina|      NE|           5|\r\n",
      "|       Wyoming|      NW|           2|\r\n",
      "+--------------+--------+------------+\r\n",
      "only showing top 100 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P5/p5.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
