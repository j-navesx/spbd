{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Spark DataFrames Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all scripts executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"chmod: changing permissions of './P1/spark_df_p1.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P2/spark_df_p2.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P3/spark_df_p3.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P4/spark_df_p4.py': Operation not permitted\",\n",
       " \"chmod: changing permissions of './P5/spark_df_p5.py': Operation not permitted\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!chmod a+x ./*/*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./*/*.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./P1/spark_df_p1.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    # files\n",
    "    lines_states = sc.textFile('../data/epa_hap_daily_summary-small.csv')\n",
    "    #lines = sc.textFile('log.csv')\n",
    "\n",
    "    # file mapping\n",
    "    logRows = lines_states.filter( lambda line : len(line) > 0)    \\\n",
    "                    .zipWithIndex() \\\n",
    "                    .filter( lambda x: x[1] > 0) \\\n",
    "                    .map(lambda x: x[0]) \\\n",
    "                    .map( lambda line: line.split(',')) \\\n",
    "                    .map( lambda arr : Row( state = arr[24], countyCode = arr[1], site_num = arr[2]))    \n",
    "    \n",
    "    # Dataframe creation\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    #logRowsDF = spark.createDataFrame( logRows )\n",
    "    #logRowsDF = logRowsDF.distinct() # Makes sure we are using different monitors\n",
    "    \n",
    "    logRows2DF = logRowsDF.select('state','countyCode','site_num').distinct().groupBy('state')\\\n",
    "                                                                         .agg(count('site_num').alias('Nr of Monitors'))\\\n",
    "                                                                         .sort('Nr of Monitors', ascending = False)   \n",
    "    logRows2DF.show(100,truncate=50)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:10:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 3:=========>                                              (35 + 9) / 200]',\n",
       " '[Stage 3:===============>                                        (54 + 8) / 200]',\n",
       " '[Stage 3:=====================>                                  (75 + 8) / 200]',\n",
       " '[Stage 3:===========================>                           (101 + 8) / 200]',\n",
       " '[Stage 3:==================================>                    (127 + 9) / 200]',\n",
       " '[Stage 3:==========================================>            (156 + 8) / 200]',\n",
       " '[Stage 3:=================================================>     (179 + 8) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m16.302s',\n",
       " 'user\\t0m1.146s',\n",
       " 'sys\\t0m1.010s']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P1/spark_df_p1.py > ./P1/p1.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\r\n",
      "|               state|Nr of Monitors|\r\n",
      "+--------------------+--------------+\r\n",
      "|          California|           162|\r\n",
      "|               Texas|           132|\r\n",
      "|           Minnesota|            94|\r\n",
      "|                Ohio|            89|\r\n",
      "|            Michigan|            84|\r\n",
      "|            New York|            66|\r\n",
      "|      South Carolina|            64|\r\n",
      "|        Pennsylvania|            60|\r\n",
      "|             Montana|            60|\r\n",
      "|             Indiana|            52|\r\n",
      "|            Colorado|            51|\r\n",
      "|            Illinois|            50|\r\n",
      "|             Florida|            50|\r\n",
      "|      North Carolina|            49|\r\n",
      "|          Washington|            42|\r\n",
      "|           Louisiana|            40|\r\n",
      "|             Arizona|            38|\r\n",
      "|              Kansas|            37|\r\n",
      "|             Georgia|            34|\r\n",
      "|              Oregon|            31|\r\n",
      "|            Kentucky|            30|\r\n",
      "|             Alabama|            28|\r\n",
      "|           Tennessee|            27|\r\n",
      "|          New Jersey|            24|\r\n",
      "|           Wisconsin|            24|\r\n",
      "|             Vermont|            22|\r\n",
      "|               Maine|            21|\r\n",
      "|         Mississippi|            21|\r\n",
      "|            Oklahoma|            20|\r\n",
      "|            Virginia|            20|\r\n",
      "|       Massachusetts|            19|\r\n",
      "|                Iowa|            18|\r\n",
      "|            Maryland|            17|\r\n",
      "|          New Mexico|            17|\r\n",
      "|               Idaho|            17|\r\n",
      "|   Country Of Mexico|            16|\r\n",
      "|         Connecticut|            15|\r\n",
      "|            Missouri|            15|\r\n",
      "|                Utah|            12|\r\n",
      "|        Rhode Island|            12|\r\n",
      "|              Alaska|            12|\r\n",
      "|       New Hampshire|            12|\r\n",
      "|       West Virginia|            10|\r\n",
      "|            Arkansas|            10|\r\n",
      "|             Wyoming|             9|\r\n",
      "|              Nevada|             8|\r\n",
      "|        North Dakota|             7|\r\n",
      "|        South Dakota|             7|\r\n",
      "|         Puerto Rico|             6|\r\n",
      "|            Delaware|             6|\r\n",
      "|            Nebraska|             6|\r\n",
      "|              Hawaii|             5|\r\n",
      "|District Of Columbia|             5|\r\n",
      "|      Virgin Islands|             4|\r\n",
      "+--------------------+--------------+\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P1/p1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./P2/spark_df_p2.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( county_name = arr[25], state_code = arr[0], county_code = arr[1], arithmetic_mean = float(arr[16])))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF.createOrReplaceTempView(\"log\")\n",
    "    \n",
    "    # Necessary computations to solve the problem\n",
    "    finalDF = logRowsDF.withColumn('county', (col('state_code')+col('county_code'))) \\\n",
    "                    .drop('state_code') \\\n",
    "                    .drop('county_code') \\\n",
    "                    .groupBy('county','county_name').avg('arithmetic_mean') \\\n",
    "                    .withColumnRenamed('avg(arithmetic_mean)', 'pollutant_levels') \\\n",
    "                    .orderBy(col('pollutant_levels').desc()) \\\n",
    "                    .drop('county') \\\n",
    "\n",
    "    finalDF.show(20)\n",
    "    sc.stop()\n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:10:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 0:============================================>              (3 + 1) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 3:===========================>                           (100 + 8) / 200]',\n",
       " '[Stage 3:========================================>              (146 + 9) / 200]',\n",
       " '[Stage 3:=====================================================> (193 + 7) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m17.304s',\n",
       " 'user\\t0m1.182s',\n",
       " 'sys\\t0m1.015s']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P2/spark_df_p2.py > ./P2/p2.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\r\n",
      "|    county_name|  pollutant_levels|\r\n",
      "+---------------+------------------+\r\n",
      "|         Tipton|            2556.0|\r\n",
      "|         Nassau|              19.0|\r\n",
      "|     Columbiana| 7.385690735785953|\r\n",
      "|           Park| 5.611212121212121|\r\n",
      "|CHIHUAHUA STATE|         4.5121875|\r\n",
      "|       Caldwell| 4.116666666666667|\r\n",
      "|          Kings|3.9843770491803276|\r\n",
      "|         Madera|            3.7393|\r\n",
      "|       Franklin|3.3499999999999996|\r\n",
      "|      Jefferson|              3.07|\r\n",
      "|        Oakland| 2.888877848101266|\r\n",
      "|           Lake| 2.879328647058823|\r\n",
      "|          Duval|2.7794603978494625|\r\n",
      "|      Middlesex|2.6500000000000004|\r\n",
      "|         Kearny|2.3753333333333333|\r\n",
      "|          Bucks|2.3674999999999997|\r\n",
      "|San Luis Obispo|2.3333333333333335|\r\n",
      "|      Edgecombe|             2.325|\r\n",
      "|         Pawnee|2.2941176470588234|\r\n",
      "|    Westchester|          2.239375|\r\n",
      "+---------------+------------------+\r\n",
      "only showing top 20 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P2/p2.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./P3/spark_df_p3.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr : Row( Year = arr[11][:4], State = arr[24], Arithmetic_mean = float(arr[16])))\n",
    "    \n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    logRows2DF = logRowsDF.select('State','Year','Arithmetic_mean')\\\n",
    "                            .groupBy('Year','State')\\\n",
    "                            .avg('Arithmetic_mean').withColumnRenamed('avg(Arithmetic_mean)','Avg Pollutants')\\\n",
    "                            .orderBy(['Year','Avg Pollutants'], ascending=[1,0])\n",
    "                                                                    \n",
    "    \n",
    "    logRows2DF.show(200,truncate=50)\n",
    "    \n",
    "    sc.stop()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:11:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 0:==============>                                            (1 + 3) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 2:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 2:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 3:==============>                                         (52 + 9) / 200]',\n",
       " '[Stage 3:=======================>                               (84 + 11) / 200]',\n",
       " '[Stage 3:===============================>                      (118 + 10) / 200]',\n",
       " '[Stage 3:=============================================>         (167 + 8) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m16.751s',\n",
       " 'user\\t0m1.281s',\n",
       " 'sys\\t0m0.809s']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P3/spark_df_p3.py > ./P3/p3.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------------------+\r\n",
      "|Year|               State|       Avg Pollutants|\r\n",
      "+----+--------------------+---------------------+\r\n",
      "|1990|           Tennessee|   170.40093066666665|\r\n",
      "|1990|             Indiana|    4.098978378378379|\r\n",
      "|1990|       Massachusetts|   3.0246823529411766|\r\n",
      "|1990|             Montana|   2.0686790073529413|\r\n",
      "|1990|               Texas|   1.4824716546762589|\r\n",
      "|1990|            New York|    1.362972027972028|\r\n",
      "|1990|             Florida|   1.2765626315789476|\r\n",
      "|1990|              Kansas|   1.1408154574132492|\r\n",
      "|1990|           Louisiana|   0.9145945945945947|\r\n",
      "|1990|            Virginia|   0.8451416666666666|\r\n",
      "|1990|District Of Columbia|   0.8261508196721312|\r\n",
      "|1990|      South Carolina|   0.5598140350877193|\r\n",
      "|1990|          California|  0.41153099836333895|\r\n",
      "|1990|           Minnesota|             0.306488|\r\n",
      "|1990|          New Jersey|   0.2933352941176471|\r\n",
      "|1990|            Illinois|  0.14575701219512197|\r\n",
      "|1990|                Ohio|             0.135716|\r\n",
      "|1990|        Pennsylvania|            0.1059625|\r\n",
      "|1990|                Iowa|               0.0332|\r\n",
      "|1990|             Alabama|             0.024325|\r\n",
      "|1990|      North Carolina|               0.0143|\r\n",
      "|1990|         Puerto Rico|              0.01005|\r\n",
      "|1990|             Georgia| 0.008366666666666666|\r\n",
      "|1990|         Connecticut|               0.0081|\r\n",
      "|1990|            Michigan| 0.006559896373056996|\r\n",
      "|1990|            Missouri|               0.0056|\r\n",
      "|1990|         Mississippi|0.0026666666666666666|\r\n",
      "|1990|            Colorado|0.0021623741007194244|\r\n",
      "|1990|               Maine| 9.789285714285713E-4|\r\n",
      "|1990|             Arizona| 8.620134228187919E-4|\r\n",
      "|1990|              Oregon| 8.596296296296297E-4|\r\n",
      "|1990|          New Mexico| 8.222222222222222E-4|\r\n",
      "|1990|                Utah| 7.970588235294118E-4|\r\n",
      "|1990|             Wyoming| 6.045454545454545E-4|\r\n",
      "|1990|          Washington| 5.974999999999999E-4|\r\n",
      "|1990|        South Dakota|             5.705E-4|\r\n",
      "|1990|              Alaska|4.4208333333333334E-4|\r\n",
      "|1990|              Nevada|4.2080000000000004E-4|\r\n",
      "|1990|              Hawaii|1.9703703703703704E-4|\r\n",
      "|1990|           Wisconsin|                  0.0|\r\n",
      "|1990|            Oklahoma|                  0.0|\r\n",
      "|1990|       West Virginia|                  0.0|\r\n",
      "|1990|      Virgin Islands|                  0.0|\r\n",
      "|1991|             Indiana|   1.6614237288135594|\r\n",
      "|1991|           Louisiana|   1.2000000000000002|\r\n",
      "|1991|          New Jersey|   1.0485551612903226|\r\n",
      "|1991|              Kansas|   0.9181956521739129|\r\n",
      "|1991|            New York|   0.8852830188679243|\r\n",
      "|1991|      South Carolina|   0.7982056737588653|\r\n",
      "|1991|             Montana|   0.7670011501597445|\r\n",
      "|1991|District Of Columbia|   0.6772853968253968|\r\n",
      "|1991|          California|   0.6735362628865981|\r\n",
      "|1991|            Maryland|  0.31901639344262295|\r\n",
      "|1991|            Michigan|   0.3136863049853373|\r\n",
      "|1991|               Texas|    0.280053185840708|\r\n",
      "|1991|           Tennessee|   0.2561980769230769|\r\n",
      "|1991|            Virginia|          0.236509875|\r\n",
      "|1991|        Pennsylvania|  0.21666666666666667|\r\n",
      "|1991|             Florida|  0.19555000000000003|\r\n",
      "|1991|           Minnesota|  0.15858126373626374|\r\n",
      "|1991|           Wisconsin|              0.11625|\r\n",
      "|1991|            Illinois| 0.045304637681159415|\r\n",
      "|1991|            Colorado|  0.00240126213592233|\r\n",
      "|1991|             Vermont| 0.001559090909090909|\r\n",
      "|1991|               Maine|0.0013065217391304347|\r\n",
      "|1991|      Virgin Islands| 9.576470588235293E-4|\r\n",
      "|1991|             Arizona| 8.828402366863905E-4|\r\n",
      "|1991|            Kentucky| 8.585714285714285E-4|\r\n",
      "|1991|             Georgia|            8.3875E-4|\r\n",
      "|1991|          Washington|            7.1375E-4|\r\n",
      "|1991|              Hawaii|  6.80754716981132E-4|\r\n",
      "|1991|       West Virginia| 5.466666666666667E-4|\r\n",
      "|1991|              Nevada|4.8263157894736837E-4|\r\n",
      "|1991|              Oregon| 4.138095238095238E-4|\r\n",
      "|1991|          New Mexico|3.9866666666666664E-4|\r\n",
      "|1991|                Utah|3.3780487804878053E-4|\r\n",
      "|1991|             Wyoming|             3.258E-4|\r\n",
      "|1991|        South Dakota| 2.883333333333333E-4|\r\n",
      "|1991|              Alaska|1.7600000000000002E-4|\r\n",
      "|1991|                Ohio|                  0.0|\r\n",
      "|1991|            Arkansas|                  0.0|\r\n",
      "|1992|            Illinois|    3.911825163398692|\r\n",
      "|1992|             Indiana|   2.6606363636363635|\r\n",
      "|1992|       Massachusetts|   2.1174999999999997|\r\n",
      "|1992|         Connecticut|               1.5926|\r\n",
      "|1992|               Texas|   1.3476657142857142|\r\n",
      "|1992|             Montana|   0.8909444329896907|\r\n",
      "|1992|              Kansas|   0.8702934232715009|\r\n",
      "|1992|           Wisconsin|   0.8223666666666666|\r\n",
      "|1992|            New York|   0.8076014814814815|\r\n",
      "|1992|          California|   0.7356345798816569|\r\n",
      "|1992|            Michigan|   0.5887766109785203|\r\n",
      "|1992|District Of Columbia|  0.41693447368421055|\r\n",
      "|1992|           Minnesota|   0.3721225333333333|\r\n",
      "|1992|            Maryland|   0.3572955974842767|\r\n",
      "|1992|          New Jersey|   0.3177194545454545|\r\n",
      "|1992|             Florida|              0.27671|\r\n",
      "|1992|             Alabama|  0.22074424242424245|\r\n",
      "|1992|           Louisiana|                 0.16|\r\n",
      "|1992|           Tennessee| 0.009468055555555556|\r\n",
      "|1992|            Colorado|  0.00671141935483871|\r\n",
      "|1992|      South Carolina|0.0027575757575757577|\r\n",
      "|1992|      Virgin Islands| 0.001658333333333333|\r\n",
      "|1992|            Kentucky| 0.001314090909090909|\r\n",
      "|1992|              Oregon| 9.852380952380952E-4|\r\n",
      "|1992|       West Virginia| 8.235135135135136E-4|\r\n",
      "|1992|             Georgia| 8.029166666666666E-4|\r\n",
      "|1992|               Maine|  6.71111111111111E-4|\r\n",
      "|1992|            Virginia| 5.474074074074074E-4|\r\n",
      "|1992|             Arizona| 5.238461538461539E-4|\r\n",
      "|1992|        South Dakota| 4.973913043478262E-4|\r\n",
      "|1992|             Vermont| 4.969565217391304E-4|\r\n",
      "|1992|            Arkansas| 4.702272727272727E-4|\r\n",
      "|1992|             Wyoming| 4.655813953488372E-4|\r\n",
      "|1992|              Nevada| 3.333333333333333E-4|\r\n",
      "|1992|          New Mexico|3.1535714285714283E-4|\r\n",
      "|1992|                Utah|2.8365384615384613E-4|\r\n",
      "|1992|          Washington|             2.696E-4|\r\n",
      "|1992|               Idaho| 2.343478260869565E-4|\r\n",
      "|1992|              Hawaii|1.8947368421052632E-4|\r\n",
      "|1992|              Alaska| 1.026923076923077E-4|\r\n",
      "|1992|                Ohio|                  0.0|\r\n",
      "|1993|       Massachusetts|    4.305833285714285|\r\n",
      "|1993|         Connecticut|   3.0975461538461535|\r\n",
      "|1993|             Indiana|   2.8972258064516128|\r\n",
      "|1993|            Delaware|             2.723077|\r\n",
      "|1993|        Pennsylvania|   2.5750862068965517|\r\n",
      "|1993|   Country Of Mexico|                 2.38|\r\n",
      "|1993|           Louisiana|   2.1176116250000003|\r\n",
      "|1993|             Alabama|   1.3672750000000002|\r\n",
      "|1993|          New Jersey|          0.869104375|\r\n",
      "|1993|            New York|   0.8594253164556963|\r\n",
      "|1993|               Texas|   0.8399963883495148|\r\n",
      "|1993|            Maryland|    0.815136698630137|\r\n",
      "|1993|District Of Columbia|   0.6556622222222221|\r\n",
      "|1993|                Ohio|   0.6267321428571428|\r\n",
      "|1993|          California|   0.6142810358974361|\r\n",
      "|1993|              Kansas|   0.5591933333333334|\r\n",
      "|1993|            Michigan|   0.4977501285347044|\r\n",
      "|1993|            Illinois|   0.2588674121405751|\r\n",
      "|1993|           Wisconsin|              0.24084|\r\n",
      "|1993|             Vermont|  0.13674265306122446|\r\n",
      "|1993|             Georgia|  0.12912760714285715|\r\n",
      "|1993|           Minnesota|  0.10624294117647057|\r\n",
      "|1993|               Maine|  0.07480310344827587|\r\n",
      "|1993|           Tennessee|   0.0737856896551724|\r\n",
      "|1993|      South Carolina| 0.005991228070175439|\r\n",
      "|1993|            Colorado|            0.0055075|\r\n",
      "|1993|      Virgin Islands|0.0010107407407407407|\r\n",
      "|1993|             Montana| 9.522857142857144E-4|\r\n",
      "|1993|       West Virginia|          9.090625E-4|\r\n",
      "|1993|             Arizona|              8.74E-4|\r\n",
      "|1993|          Washington|  7.83859649122807E-4|\r\n",
      "|1993|            Virginia| 7.591666666666667E-4|\r\n",
      "|1993|            Kentucky| 7.058064516129032E-4|\r\n",
      "|1993|             Florida| 6.423684210526316E-4|\r\n",
      "|1993|            Arkansas|4.2833333333333335E-4|\r\n",
      "|1993|               Idaho| 4.044186046511628E-4|\r\n",
      "|1993|          New Mexico|3.9781250000000005E-4|\r\n",
      "|1993|                Utah|3.5382978723404255E-4|\r\n",
      "|1993|              Nevada|3.0183673469387756E-4|\r\n",
      "|1993|        South Dakota| 2.488888888888889E-4|\r\n",
      "|1993|              Oregon|2.3999999999999998E-4|\r\n",
      "|1993|             Wyoming| 2.142105263157895E-4|\r\n",
      "|1993|              Alaska|1.5647058823529413E-4|\r\n",
      "|1993|              Hawaii|          1.409375E-4|\r\n",
      "|1994|       Massachusetts|   3.4609906122448977|\r\n",
      "|1994|        Rhode Island|   3.3635714000000005|\r\n",
      "|1994|           Wisconsin|   2.9504833333333336|\r\n",
      "|1994|            Delaware|             2.082609|\r\n",
      "|1994|        Pennsylvania|   1.8054421904761901|\r\n",
      "|1994|             Alabama|   1.7862802040816326|\r\n",
      "|1994|          New Jersey|    1.007733870967742|\r\n",
      "|1994|               Texas|   0.9557173289085547|\r\n",
      "|1994|            Maryland|   0.9504381768292685|\r\n",
      "|1994|            New York|   0.9327445387931034|\r\n",
      "|1994|             Vermont|   0.9107219230769231|\r\n",
      "|1994|         Connecticut|   0.9070260000000001|\r\n",
      "|1994|           Louisiana|   0.9058858267716535|\r\n",
      "|1994|             Indiana|   0.8797972972972972|\r\n",
      "|1994|              Kansas|    0.829511201629328|\r\n",
      "|1994|                Ohio|   0.7074822222222221|\r\n",
      "|1994|          California|   0.6082877907869484|\r\n",
      "|1994|            Michigan|   0.5735232835820897|\r\n",
      "|1994|           Minnesota|    0.555922987012987|\r\n",
      "|1994|District Of Columbia|   0.5384301590909091|\r\n",
      "|1994|            Illinois|   0.2914373401534527|\r\n",
      "|1994|             Georgia|   0.2874106666666666|\r\n",
      "|1994|               Maine|          0.188303025|\r\n",
      "|1994|      South Carolina|  0.08517265565438376|\r\n",
      "|1994|            Virginia|  0.07424926829268293|\r\n",
      "|1994|           Tennessee|            0.0174475|\r\n",
      "|1994|             Montana|0.0019819565217391306|\r\n",
      "|1994|             Arizona|0.0014466666666666666|\r\n",
      "|1994|      Virgin Islands|           0.00122375|\r\n",
      "|1994|            Kentucky| 0.001182941176470588|\r\n",
      "|1994|       West Virginia|0.0010313793103448278|\r\n",
      "|1994|             Florida| 8.884210526315789E-4|\r\n",
      "|1994|          New Mexico| 7.054054054054053E-4|\r\n",
      "|1994|      North Carolina|               6.6E-4|\r\n",
      "+----+--------------------+---------------------+\r\n",
      "only showing top 200 rows\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P3/p3.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./P4/spark_df_p4.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    # files\n",
    "    lines_states = sc.textFile('../data/usa_states.csv')\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv')\n",
    "\n",
    "    # file mapping\n",
    "    logRows_states = lines_states.filter( lambda line : len(line) > 0)    \\\n",
    "                    .zipWithIndex() \\\n",
    "                    .filter( lambda x: x[1] > 0) \\\n",
    "                    .map(lambda x: x[0]) \\\n",
    "                    .map( lambda line: line.split(',')) \\\n",
    "                    .map( lambda arr : Row( state = arr[0], name = arr[1], minLat = float(arr[2]), \\\n",
    "                                            maxLat = float(arr[3]), minLon = float(arr[4]), \\\n",
    "                                            maxLon = float(arr[5])))\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr: Row( name = arr[24], county = arr[1], siteNum = arr[2], Lat = float(\"{:.3f}\".format(float(arr[5]))), Lon = float(\"{:.3f}\".format(float(arr[6]))) ) )    \n",
    "    \n",
    "    # Dataframe creation\n",
    "    logRowsStatesDF = spark.createDataFrame( logRows_states )\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    logRowsDF = logRowsDF.distinct() # Makes sure we are using different monitors\n",
    "\n",
    "    # Necessary computations to solve the problem\n",
    "    finalDF = logRowsStatesDF.withColumn('center_Lat', (col('minLat')+col('maxLat'))/2 ) \\\n",
    "                        .withColumn('center_Lon', (col('minLon')+col('maxLon'))/2 ) \\\n",
    "                        .drop('minLon') \\\n",
    "                        .drop('maxLon') \\\n",
    "                        .drop('minLat') \\\n",
    "                        .drop('maxLat') \\\n",
    "                        .drop('state') \\\n",
    "                        .drop('county') \\\n",
    "                        .drop('siteNum') \\\n",
    "                        .join(logRowsDF, 'name') \\\n",
    "                        .withColumn('distance', sqrt( pow((col('Lat')-col('center_Lat'))*111,2) + pow((col('Lon')-col('center_Lon'))*111,2) ) ) \\\n",
    "                        .groupBy('name').avg('distance')\n",
    "    \n",
    "    finalDF.show(54)\n",
    "\n",
    "    sc.stop()\n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 2) / 2]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 1:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 1:=============================>                             (2 + 2) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 4:>                  (0 + 4) / 4][Stage 6:>                  (0 + 2) / 2]',\n",
       " '[Stage 4:>                  (0 + 4) / 4][Stage 6:=========>         (1 + 1) / 2]',\n",
       " '[Stage 4:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 4:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 5:================>                                       (60 + 8) / 200]',\n",
       " '[Stage 5:======================>                                (81 + 12) / 200]',\n",
       " '[Stage 5:=============================>                         (107 + 8) / 200]',\n",
       " '[Stage 5:====================================>                  (132 + 8) / 200]',\n",
       " '[Stage 5:============================================>          (163 + 8) / 200]',\n",
       " '[Stage 5:====================================================>  (192 + 8) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m21.783s',\n",
       " 'user\\t0m1.481s',\n",
       " 'sys\\t0m0.923s']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P4/spark_df_p4.py > ./P4/p4.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\r\n",
      "|          name|     avg(distance)|\r\n",
      "+--------------+------------------+\r\n",
      "|          Utah|184.91876919510068|\r\n",
      "|        Hawaii|155.72848584906362|\r\n",
      "|     Minnesota|195.06726533715846|\r\n",
      "|          Ohio|175.74265339396337|\r\n",
      "|      Arkansas|157.84229127028243|\r\n",
      "|        Oregon| 268.8565041755878|\r\n",
      "|         Texas| 512.0338063321818|\r\n",
      "|  North Dakota| 248.4324915397246|\r\n",
      "|  Pennsylvania|251.41090880145097|\r\n",
      "|   Connecticut| 49.99224954412998|\r\n",
      "|      Nebraska|307.13572198974265|\r\n",
      "|       Vermont| 521.9872635614968|\r\n",
      "|        Nevada| 326.2811669321348|\r\n",
      "|   Puerto Rico| 32.73340559951166|\r\n",
      "|    Washington| 219.9837579015187|\r\n",
      "|      Illinois|435.24508277080713|\r\n",
      "|      Oklahoma|236.88430984912995|\r\n",
      "|Virgin Islands| 73.48448478701415|\r\n",
      "|      Delaware|51.577754815182395|\r\n",
      "|        Alaska| 603.7055510312542|\r\n",
      "|    New Mexico|183.17300873742957|\r\n",
      "| West Virginia| 144.4948207089262|\r\n",
      "|      Missouri| 230.0545549271448|\r\n",
      "|  Rhode Island|22.194351406681417|\r\n",
      "|       Georgia|185.57120999020043|\r\n",
      "|       Montana| 284.9444358122064|\r\n",
      "|      Michigan| 329.4440408970379|\r\n",
      "|      Virginia|1167.8266779336668|\r\n",
      "|North Carolina|179.09364428919878|\r\n",
      "|       Wyoming|283.65083457659074|\r\n",
      "|        Kansas| 292.0781329241548|\r\n",
      "|    New Jersey| 80.74117031360068|\r\n",
      "|      Maryland|  89.2999009259314|\r\n",
      "|       Alabama|164.61144485299064|\r\n",
      "|       Arizona|178.89012459120053|\r\n",
      "|          Iowa| 206.5833608417488|\r\n",
      "| Massachusetts| 92.34794730969004|\r\n",
      "|      Kentucky|223.76963593367316|\r\n",
      "|     Louisiana|178.22826309633484|\r\n",
      "|   Mississippi|174.23332103586176|\r\n",
      "| New Hampshire|116.80752882032644|\r\n",
      "|     Tennessee|243.85649176309732|\r\n",
      "|       Florida|336.54870825716444|\r\n",
      "|       Indiana| 177.7417956680167|\r\n",
      "|         Idaho| 289.6288745980761|\r\n",
      "|South Carolina|131.48946099874948|\r\n",
      "|  South Dakota| 365.8476206180489|\r\n",
      "|    California| 329.6869380718333|\r\n",
      "|      New York| 283.5204283804173|\r\n",
      "|     Wisconsin|203.00348167333075|\r\n",
      "|      Colorado|180.25508838992187|\r\n",
      "|         Maine|167.76358421309246|\r\n",
      "+--------------+------------------+\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P4/p4.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./P5/spark_df_p5.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    # files\n",
    "    lines_states = sc.textFile('../data/usa_states.csv')\n",
    "    lines = sc.textFile('../data/epa_hap_daily_summary-small.csv') # Change the name of the file to what you have it named here\n",
    "    \n",
    "    # file mapping\n",
    "    logRows_states = lines_states.filter( lambda line : len(line) > 0)    \\\n",
    "                    .zipWithIndex() \\\n",
    "                    .filter( lambda x: x[1] > 0) \\\n",
    "                    .map(lambda x: x[0]) \\\n",
    "                    .map( lambda line: line.split(',')) \\\n",
    "                    .map( lambda arr : Row( state = arr[0], name = arr[1], centerLat = (float(arr[2]) + float(arr[3]))/2, \\\n",
    "                                            centerLon = (float(arr[4]) + float(arr[5]))/2))\n",
    "    logRows = lines.filter( lambda line: len(line) > 0) \\\n",
    "                     .zipWithIndex() \\\n",
    "                     .filter( lambda x: x[1] > 0) \\\n",
    "                     .map(lambda x: x[0]) \\\n",
    "                     .map( lambda line: line.split(',')) \\\n",
    "                     .map( lambda arr: Row( name = arr[24], countyCode = arr[1], siteNum = arr[2], lat = float(\"{:.3f}\".format(float(arr[5]))), lon = float(\"{:.3f}\".format(float(arr[6]))) ) )  \n",
    "    \n",
    "    # Creates the dataframes\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    logRows_statesDF = spark.createDataFrame( logRows_states )\n",
    "    \n",
    "    \n",
    "    logRowsDF = logRowsDF.select('name','countyCode','siteNum','lat','lon').distinct()\n",
    "    \n",
    "    logRowsDF = logRowsDF.select('name','lat','lon')\n",
    "    \n",
    "    \n",
    "    joinedDF = logRowsDF.join(logRows_statesDF,logRowsDF.name == logRows_statesDF.name,\"inner\" )\n",
    "    \n",
    "    MonitorDF = joinedDF.select((logRowsDF.name).alias('name'),\\\n",
    "                                (logRowsDF.lat).alias('lat'),\\\n",
    "                                (logRowsDF.lon).alias('lon'),\\\n",
    "                                (logRows_statesDF.centerLat).alias('centerLat'),\\\n",
    "                                (logRows_statesDF.centerLon).alias('centerLon')\\\n",
    "                               )\n",
    "    \n",
    "    \n",
    "    MonitorDF = MonitorDF.withColumn(\"quadrant\", when((col(\"lat\") < col(\"centerLat\")) & (col(\"lon\") < col(\"centerLon\")),'NW')\\\n",
    "                                                 .when((col(\"lat\") < col(\"centerLat\")) & (col(\"lon\") > col(\"centerLon\")),'NE')\\\n",
    "                                                 .when((col(\"lat\") > col(\"centerLat\")) & (col(\"lon\") < col(\"centerLon\")),'SW')\\\n",
    "                                                 .when((col(\"lat\") > col(\"centerLat\")) & (col(\"lon\") > col(\"centerLon\")),'SE')\n",
    "                                                 .otherwise('Center or Borders')\n",
    "                                    )                              \n",
    "    \n",
    "    FinalMonitorDF = MonitorDF.groupBy('name','quadrant').agg(count('quadrant').alias('Nr of Monitors')).sort('name', ascending = True)\n",
    "    \n",
    "    FinalMonitorDF.show(400)\n",
    "    \n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21/12/23 23:12:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
       " \"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " 'Setting default log level to \"WARN\".',\n",
       " 'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
       " '',\n",
       " '[Stage 0:>                                                          (0 + 2) / 2]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 1:==============>                                            (1 + 3) / 4]',\n",
       " '                                                                                ',\n",
       " '',\n",
       " '[Stage 4:>                  (0 + 2) / 2][Stage 5:>                  (0 + 4) / 4]',\n",
       " '[Stage 5:>                                                          (0 + 4) / 4]',\n",
       " '[Stage 5:==============>                                            (1 + 3) / 4]',\n",
       " '[Stage 6:==============>                                         (52 + 8) / 200]',\n",
       " '[Stage 6:======================>                                 (79 + 8) / 200]',\n",
       " '[Stage 6:=============================>                         (107 + 9) / 200]',\n",
       " '[Stage 6:====================================>                  (132 + 8) / 200]',\n",
       " '[Stage 6:============================================>          (162 + 8) / 200]',\n",
       " '[Stage 6:=====================================================> (193 + 7) / 200]',\n",
       " '[Stage 7:================>                                      (60 + 10) / 200]',\n",
       " '[Stage 7:============================>                          (104 + 8) / 200]',\n",
       " '[Stage 7:=======================================>               (144 + 8) / 200]',\n",
       " '[Stage 7:================================================>      (178 + 9) / 200]',\n",
       " '                                                                                ',\n",
       " 'real\\t0m19.339s',\n",
       " 'user\\t0m1.204s',\n",
       " 'sys\\t0m1.030s']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!time python ./P5/spark_df_p5.py > ./P5/p5.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------------+\r\n",
      "|          name|quadrant|Nr of Monitors|\r\n",
      "+--------------+--------+--------------+\r\n",
      "|       Alabama|      NE|             4|\r\n",
      "|       Alabama|      SW|            14|\r\n",
      "|       Alabama|      NW|             7|\r\n",
      "|       Alabama|      SE|             4|\r\n",
      "|        Alaska|      NE|             2|\r\n",
      "|        Alaska|      NW|             3|\r\n",
      "|        Alaska|      SW|             3|\r\n",
      "|        Alaska|      SE|             4|\r\n",
      "|       Arizona|      NE|            16|\r\n",
      "|       Arizona|      SE|             2|\r\n",
      "|       Arizona|      NW|            10|\r\n",
      "|       Arizona|      SW|            10|\r\n",
      "|      Arkansas|      SW|             3|\r\n",
      "|      Arkansas|      SE|             2|\r\n",
      "|      Arkansas|      NW|             5|\r\n",
      "|      Arkansas|      NE|             1|\r\n",
      "|    California|      NE|            68|\r\n",
      "|    California|      NW|            15|\r\n",
      "|    California|      SW|            84|\r\n",
      "|    California|      SE|             2|\r\n",
      "|      Colorado|      SE|            24|\r\n",
      "|      Colorado|      SW|            18|\r\n",
      "|      Colorado|      NE|             4|\r\n",
      "|      Colorado|      NW|             5|\r\n",
      "|   Connecticut|      NW|             8|\r\n",
      "|   Connecticut|      SE|             5|\r\n",
      "|   Connecticut|      SW|             2|\r\n",
      "|      Delaware|      NW|             2|\r\n",
      "|      Delaware|      SW|             4|\r\n",
      "|       Florida|      SE|            27|\r\n",
      "|       Florida|      NE|            23|\r\n",
      "|       Florida|      SW|             5|\r\n",
      "|       Georgia|      SW|            20|\r\n",
      "|       Georgia|      SE|             4|\r\n",
      "|       Georgia|      NW|             5|\r\n",
      "|       Georgia|      NE|             5|\r\n",
      "|        Hawaii|      SE|             2|\r\n",
      "|        Hawaii|      SW|             2|\r\n",
      "|        Hawaii|      NE|             1|\r\n",
      "|         Idaho|      NW|             7|\r\n",
      "|         Idaho|      SW|             7|\r\n",
      "|         Idaho|      NE|             3|\r\n",
      "|      Illinois|      SE|            32|\r\n",
      "|      Illinois|      SW|             2|\r\n",
      "|      Illinois|      NW|            15|\r\n",
      "|      Illinois|      NE|             1|\r\n",
      "|       Indiana|      NE|             8|\r\n",
      "|       Indiana|      SW|            18|\r\n",
      "|       Indiana|      SE|            18|\r\n",
      "|       Indiana|      NW|             8|\r\n",
      "|          Iowa|      NE|             8|\r\n",
      "|          Iowa|      NW|             4|\r\n",
      "|          Iowa|      SE|             6|\r\n",
      "|        Kansas|      SW|             2|\r\n",
      "|        Kansas|      SE|            18|\r\n",
      "|        Kansas|      NE|             9|\r\n",
      "|        Kansas|      NW|             8|\r\n",
      "|      Kentucky|      NW|            14|\r\n",
      "|      Kentucky|      SE|            12|\r\n",
      "|      Kentucky|      SW|             3|\r\n",
      "|      Kentucky|      NE|             2|\r\n",
      "|     Louisiana|      NE|            34|\r\n",
      "|     Louisiana|      NW|             1|\r\n",
      "|     Louisiana|      SW|             5|\r\n",
      "|         Maine|      SE|             2|\r\n",
      "|         Maine|      SW|             1|\r\n",
      "|         Maine|      NE|             6|\r\n",
      "|         Maine|      NW|            12|\r\n",
      "|      Maryland|      SE|            16|\r\n",
      "|      Maryland|      SW|             1|\r\n",
      "| Massachusetts|      SE|            11|\r\n",
      "| Massachusetts|      NE|             4|\r\n",
      "| Massachusetts|      SW|             4|\r\n",
      "|      Michigan|      NE|            69|\r\n",
      "|      Michigan|      SE|            11|\r\n",
      "|      Michigan|      SW|             3|\r\n",
      "|      Michigan|      NW|             1|\r\n",
      "|     Minnesota|      NW|            21|\r\n",
      "|     Minnesota|      SE|            11|\r\n",
      "|     Minnesota|      NE|            50|\r\n",
      "|     Minnesota|      SW|            12|\r\n",
      "|   Mississippi|      SE|             4|\r\n",
      "|   Mississippi|      NE|            12|\r\n",
      "|   Mississippi|      NW|             5|\r\n",
      "|      Missouri|      NE|             2|\r\n",
      "|      Missouri|      SW|             3|\r\n",
      "|      Missouri|      NW|             2|\r\n",
      "|      Missouri|      SE|             9|\r\n",
      "|       Montana|      NW|            36|\r\n",
      "|       Montana|      SE|            11|\r\n",
      "|       Montana|      SW|            10|\r\n",
      "|       Montana|      NE|             4|\r\n",
      "|      Nebraska|      NE|             3|\r\n",
      "|      Nebraska|      SE|             1|\r\n",
      "|      Nebraska|      SW|             2|\r\n",
      "|        Nevada|      SW|             2|\r\n",
      "|        Nevada|      NE|             5|\r\n",
      "|        Nevada|      SE|             2|\r\n",
      "| New Hampshire|      NW|             4|\r\n",
      "| New Hampshire|      NE|             9|\r\n",
      "| New Hampshire|      SE|             1|\r\n",
      "|    New Jersey|      NE|             1|\r\n",
      "|    New Jersey|      NW|             4|\r\n",
      "|    New Jersey|      SE|            18|\r\n",
      "|    New Jersey|      SW|             1|\r\n",
      "|    New Mexico|      NE|             2|\r\n",
      "|    New Mexico|      NW|             3|\r\n",
      "|    New Mexico|      SE|             1|\r\n",
      "|    New Mexico|      SW|            12|\r\n",
      "|      New York|      NE|            42|\r\n",
      "|      New York|      SW|            16|\r\n",
      "|      New York|      SE|             5|\r\n",
      "|      New York|      NW|             3|\r\n",
      "|North Carolina|      NE|             5|\r\n",
      "|North Carolina|      SW|            19|\r\n",
      "|North Carolina|      SE|            26|\r\n",
      "|  North Dakota|      SW|             2|\r\n",
      "|  North Dakota|      NW|             3|\r\n",
      "|  North Dakota|      SE|             1|\r\n",
      "|  North Dakota|      NE|             1|\r\n",
      "|          Ohio|      NW|            34|\r\n",
      "|          Ohio|      NE|            10|\r\n",
      "|          Ohio|      SW|            15|\r\n",
      "|          Ohio|      SE|            30|\r\n",
      "|      Oklahoma|      SE|            18|\r\n",
      "|      Oklahoma|      NE|             2|\r\n",
      "|      Oklahoma|      SW|             2|\r\n",
      "|        Oregon|      SE|             3|\r\n",
      "|        Oregon|      NE|             1|\r\n",
      "|        Oregon|      NW|            13|\r\n",
      "|        Oregon|      SW|            15|\r\n",
      "|  Pennsylvania|      NE|            36|\r\n",
      "|  Pennsylvania|      NW|            18|\r\n",
      "|  Pennsylvania|      SE|             3|\r\n",
      "|  Pennsylvania|      SW|             4|\r\n",
      "|   Puerto Rico|      SE|             4|\r\n",
      "|   Puerto Rico|      NW|             1|\r\n",
      "|   Puerto Rico|      SW|             1|\r\n",
      "|  Rhode Island|      SE|            12|\r\n",
      "|  Rhode Island|      SW|             1|\r\n",
      "|South Carolina|      SW|            33|\r\n",
      "|South Carolina|      NE|            18|\r\n",
      "|South Carolina|      NW|             3|\r\n",
      "|South Carolina|      SE|            10|\r\n",
      "|  South Dakota|      NE|             4|\r\n",
      "|  South Dakota|      NW|             3|\r\n",
      "|     Tennessee|      NW|             8|\r\n",
      "|     Tennessee|      NE|             4|\r\n",
      "|     Tennessee|      SE|             8|\r\n",
      "|     Tennessee|      SW|             7|\r\n",
      "|         Texas|      NW|             3|\r\n",
      "|         Texas|      SE|            34|\r\n",
      "|         Texas|      SW|            23|\r\n",
      "|         Texas|      NE|            72|\r\n",
      "|          Utah|      NE|             3|\r\n",
      "|          Utah|      SW|             6|\r\n",
      "|          Utah|      NW|             3|\r\n",
      "|       Vermont|      SE|             1|\r\n",
      "|       Vermont|      SW|            16|\r\n",
      "|       Vermont|      NE|             1|\r\n",
      "|       Vermont|      NW|             4|\r\n",
      "|Virgin Islands|      NW|             6|\r\n",
      "|      Virginia|      NE|             8|\r\n",
      "|      Virginia|      SE|             8|\r\n",
      "|      Virginia|      NW|             4|\r\n",
      "|    Washington|      SW|            20|\r\n",
      "|    Washington|      NW|            15|\r\n",
      "|    Washington|      SE|             6|\r\n",
      "|    Washington|      NE|             2|\r\n",
      "| West Virginia|      SE|             2|\r\n",
      "| West Virginia|      NW|             5|\r\n",
      "| West Virginia|      SW|             3|\r\n",
      "|     Wisconsin|      NE|            20|\r\n",
      "|     Wisconsin|      SE|             2|\r\n",
      "|     Wisconsin|      SW|             2|\r\n",
      "|     Wisconsin|      NW|             1|\r\n",
      "|       Wyoming|      SW|             3|\r\n",
      "|       Wyoming|      SE|             2|\r\n",
      "|       Wyoming|      NW|             2|\r\n",
      "|       Wyoming|      NE|             2|\r\n",
      "+--------------+--------+--------------+\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./P5/p5.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
